{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Paraphrase Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from lxml import etree       \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import chardet\n",
    "import pymorphy2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "import json\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#необходимо для красивой печати токенов на русском\n",
    "def bprint(x):\n",
    "    for x_ in x:\n",
    "        print x_\n",
    "#     print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## The creation of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#формирую dataframe из предложений - информация о парафразах\n",
    "#Paraphrase classes: -1: non-paraphrases, 0: loose paraphrases, 1: strict paraphrases.\n",
    "##Для бинарной классификации: 0 - не парафразы, 1 - парафразы\n",
    "tree = etree.parse('paraphrases.xml')  \n",
    "root = tree.getroot() \n",
    "corpus = root[1]\n",
    "data = []\n",
    "\n",
    "for paraphrase in corpus:\n",
    "    new_pair_data = []\n",
    "    for field in paraphrase:\n",
    "        new_pair_data.append(field.text.encode('utf-8'))\n",
    "    data.append(new_pair_data)\n",
    "\n",
    "Paraphrases = pd.DataFrame(np.asarray(data), columns = ['pair_id', 'id_1', 'id_2', 'text_1', 'text_2', 'jaccard', 'class'])\n",
    "Paraphrases[['pair_id', 'id_1', 'id_2', 'jaccard', 'class']] = \\\n",
    "        Paraphrases[['pair_id', 'id_1', 'id_2', 'jaccard', 'class']].apply(pd.to_numeric)\n",
    "Paraphrases['class'] = Paraphrases['class'].apply(lambda x: 1 if x >=0 else 0)\n",
    "Paraphrases.to_csv(\"Paraphrases.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#формирую dataframe для предложений - информация о предложениях\n",
    "#Paraphrase classes: -1: non-paraphrases, 0: loose paraphrases, 1: strict paraphrases.\n",
    "tree = etree.parse('corpus.xml')  \n",
    "root = tree.getroot() \n",
    "corpus = root[1]\n",
    "data = []\n",
    "\n",
    "for sentense in corpus:\n",
    "    new_sentense = []\n",
    "    for field in sentense:\n",
    "        if field.text is None:\n",
    "            new_sentense.append(None)\n",
    "        else:\n",
    "            new_sentense.append(field.text.encode('utf-8'))\n",
    "    data.append(new_sentense)\n",
    "\n",
    "Sentences = pd.DataFrame(np.asarray(data), columns = ['id', 'text', 'agency', 'author', 'url', 'date'])\n",
    "Sentences[['id']] = Sentences[['id']].apply(pd.to_numeric) \n",
    "Sentences.to_csv(\"Sentences.csv\",index = False,  encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (7227, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_id</th>\n",
       "      <th>id_1</th>\n",
       "      <th>id_2</th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>jaccard</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>201</td>\n",
       "      <td>8159</td>\n",
       "      <td>Полицейским разрешат стрелять на поражение по ...</td>\n",
       "      <td>Полиции могут разрешить стрелять по хулиганам ...</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>202</td>\n",
       "      <td>8158</td>\n",
       "      <td>Право полицейских на проникновение в жилище ре...</td>\n",
       "      <td>Правила внесудебного проникновения полицейских...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>273</td>\n",
       "      <td>8167</td>\n",
       "      <td>Президент Египта ввел чрезвычайное положение в...</td>\n",
       "      <td>Власти Египта угрожают ввести в стране чрезвыч...</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>220</td>\n",
       "      <td>8160</td>\n",
       "      <td>Вернувшихся из Сирии россиян волнует вопрос тр...</td>\n",
       "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
       "      <td>0.324037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>8160</td>\n",
       "      <td>В Москву из Сирии вернулись 2 самолета МЧС с р...</td>\n",
       "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
       "      <td>0.606218</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pair_id  id_1  id_2                                             text_1  \\\n",
       "0        1   201  8159  Полицейским разрешат стрелять на поражение по ...   \n",
       "1        2   202  8158  Право полицейских на проникновение в жилище ре...   \n",
       "2        3   273  8167  Президент Египта ввел чрезвычайное положение в...   \n",
       "3        4   220  8160  Вернувшихся из Сирии россиян волнует вопрос тр...   \n",
       "4        5   223  8160  В Москву из Сирии вернулись 2 самолета МЧС с р...   \n",
       "\n",
       "                                              text_2   jaccard  class  \n",
       "0  Полиции могут разрешить стрелять по хулиганам ...  0.650000      1  \n",
       "1  Правила внесудебного проникновения полицейских...  0.500000      1  \n",
       "2  Власти Египта угрожают ввести в стране чрезвыч...  0.611429      1  \n",
       "3  Самолеты МЧС вывезут россиян из разрушенной Си...  0.324037      0  \n",
       "4  Самолеты МЧС вывезут россиян из разрушенной Си...  0.606218      1  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"Train data shape: \", Paraphrases.shape\n",
    "Paraphrases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences number:  (12062, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>agency</th>\n",
       "      <th>author</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Избежать \"фискального обрыва\": Сенат США подде...</td>\n",
       "      <td>РБК</td>\n",
       "      <td>None</td>\n",
       "      <td>http://top.rbc.ru/economics/01/01/2013/839229....</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>\"Фискальный обрыв\" в США временно предотвращен.</td>\n",
       "      <td>РБК</td>\n",
       "      <td>None</td>\n",
       "      <td>http://top.rbc.ru/economics/01/01/2013/839223....</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Чечня попросила националистов составить кодекс...</td>\n",
       "      <td>РБК</td>\n",
       "      <td>None</td>\n",
       "      <td>http://top.rbc.ru/society/01/01/2013/839242.shtml</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Северокорейский лидер впервые за 19 лет поздра...</td>\n",
       "      <td>РБК</td>\n",
       "      <td>None</td>\n",
       "      <td>http://top.rbc.ru/society/01/01/2013/839227.shtml</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>В Кот-Д`Ивуаре десятки человек погибли в давке...</td>\n",
       "      <td>РБК</td>\n",
       "      <td>None</td>\n",
       "      <td>http://top.rbc.ru/incidents/01/01/2013/839240....</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text agency author  \\\n",
       "0   1  Избежать \"фискального обрыва\": Сенат США подде...    РБК   None   \n",
       "1   2    \"Фискальный обрыв\" в США временно предотвращен.    РБК   None   \n",
       "2   3  Чечня попросила националистов составить кодекс...    РБК   None   \n",
       "3   4  Северокорейский лидер впервые за 19 лет поздра...    РБК   None   \n",
       "4   5  В Кот-Д`Ивуаре десятки человек погибли в давке...    РБК   None   \n",
       "\n",
       "                                                 url        date  \n",
       "0  http://top.rbc.ru/economics/01/01/2013/839229....  2013-01-01  \n",
       "1  http://top.rbc.ru/economics/01/01/2013/839223....  2013-01-01  \n",
       "2  http://top.rbc.ru/society/01/01/2013/839242.shtml  2013-01-01  \n",
       "3  http://top.rbc.ru/society/01/01/2013/839227.shtml  2013-01-01  \n",
       "4  http://top.rbc.ru/incidents/01/01/2013/839240....  2013-01-01  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"Sentences number: \", Sentences.shape\n",
    "Sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pair_id  id_1  id_2                                             text_1  \\\n",
      "0        1   201  8159  Полицейским разрешат стрелять на поражение по ...   \n",
      "1        2   202  8158  Право полицейских на проникновение в жилище ре...   \n",
      "2        3   273  8167  Президент Египта ввел чрезвычайное положение в...   \n",
      "3        4   220  8160  Вернувшихся из Сирии россиян волнует вопрос тр...   \n",
      "4        5   223  8160  В Москву из Сирии вернулись 2 самолета МЧС с р...   \n",
      "\n",
      "                                              text_2   jaccard  class  \n",
      "0  Полиции могут разрешить стрелять по хулиганам ...  0.650000      1  \n",
      "1  Правила внесудебного проникновения полицейских...  0.500000      1  \n",
      "2  Власти Египта угрожают ввести в стране чрезвыч...  0.611429      1  \n",
      "3  Самолеты МЧС вывезут россиян из разрушенной Си...  0.324037      0  \n",
      "4  Самолеты МЧС вывезут россиян из разрушенной Си...  0.606218      1  \n",
      "   id                                               text agency author  \\\n",
      "0   1  Избежать \"фискального обрыва\": Сенат США подде...    РБК    NaN   \n",
      "1   2    \"Фискальный обрыв\" в США временно предотвращен.    РБК    NaN   \n",
      "2   3  Чечня попросила националистов составить кодекс...    РБК    NaN   \n",
      "3   4  Северокорейский лидер впервые за 19 лет поздра...    РБК    NaN   \n",
      "4   5  В Кот-Д`Ивуаре десятки человек погибли в давке...    РБК    NaN   \n",
      "\n",
      "                                                 url        date  \n",
      "0  http://top.rbc.ru/economics/01/01/2013/839229....  2013-01-01  \n",
      "1  http://top.rbc.ru/economics/01/01/2013/839223....  2013-01-01  \n",
      "2  http://top.rbc.ru/society/01/01/2013/839242.shtml  2013-01-01  \n",
      "3  http://top.rbc.ru/society/01/01/2013/839227.shtml  2013-01-01  \n",
      "4  http://top.rbc.ru/incidents/01/01/2013/839240....  2013-01-01  \n"
     ]
    }
   ],
   "source": [
    "Paraphrases = pd.read_csv(\"Paraphrases.csv\")\n",
    "print Paraphrases.head()\n",
    "Sentences = pd.read_csv(\"Sentences.csv\")\n",
    "print Sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Lemmatization and constructing the inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "RusLem = pymorphy2.MorphAnalyzer()\n",
    "#обратный индес: {токен:список id предложений,в которых токен встречается}\n",
    "inverted_index = {}\n",
    "#прямой индекс: {id предложения:список токенов}\n",
    "forward_index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np_sentences = np.asarray(Sentences[['id', 'text']])\n",
    "\n",
    "for sent in np_sentences:\n",
    "    tokens = re.findall('[\\w]+',sent[1].decode(\"utf-8\").strip().lower(), re.U)\n",
    "    tokens = [RusLem.parse(token)[0].normal_form for token in tokens]\n",
    "    forward_index[sent[0]] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_inverted_index(forward_index):\n",
    "    inverted_index = {}\n",
    "    for sent_id in forward_index.keys():\n",
    "        for token in forward_index[sent_id]:\n",
    "            if token not in inverted_index:\n",
    "                inverted_index[token] = [sent_id]\n",
    "            elif sent_id not in inverted_index[token]:\n",
    "                inverted_index[token].append(sent_id)\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# bprint(np_sentences[:5, 1])\n",
    "inverted_index = create_inverted_index(forward_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(\"./forward_index\", 'wb') as res_file:\n",
    "    pickle.dump(forward_index, res_file)\n",
    "with open(\"./inverted_index\", 'wb') as res_file:\n",
    "    pickle.dump(inverted_index, res_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## String - based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25495 34424 34611]\n",
      " [25496 34596 34611]\n",
      " [25498 34488 34613]\n",
      " [25499 34614 34615]\n",
      " [25500 34616 34617]\n",
      " [25514 34622 34633]\n",
      " [25524 34566 34654]\n",
      " [25548 34519 34681]\n",
      " [25549 34565 34681]\n",
      " [25577 34584 34722]]\n"
     ]
    }
   ],
   "source": [
    "np_paraphrases = np.asarray(Paraphrases[['pair_id', 'id_1', 'id_2']])\n",
    "print np_paraphrases[len(np_paraphrases) - 10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![Image](img1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#возвращает значение признака для 1 пары предложений\n",
    "#paraphrase = [id_1, id_2]\n",
    "def get_string_feature(forward_index, paraphrase, n_gram = 1, gram_type = 'word', feat_num = 1):\n",
    "    id_1 = paraphrase[0]\n",
    "    id_2 = paraphrase[1]\n",
    "    if gram_type == 'word':\n",
    "        tokens_1 = forward_index[id_1]\n",
    "        tokens_2 = forward_index[id_2]\n",
    "        if n_gram == 1:\n",
    "            set_1 = set(tokens_1)\n",
    "            set_2 = set(tokens_2)\n",
    "        elif n_gram == 2:\n",
    "            set_1 = set([\" \".join([tokens_1[idx], tokens_1[idx + 1]]) for idx in range(len(tokens_1) - 1)])\n",
    "            set_2 = set([\" \".join([tokens_2[idx], tokens_2[idx + 1]]) for idx in range(len(tokens_2) - 1)])\n",
    "        elif n_gram == 3:\n",
    "            set_1 = set([\" \".join([tokens_1[idx], tokens_1[idx + 1], tokens_1[idx + 2]]) \\\n",
    "                                                     for idx in range(len(tokens_1) - 2)])\n",
    "            set_2 = set([\" \".join([tokens_2[idx], tokens_2[idx + 1], tokens_2[idx + 2]])\\\n",
    "                                                     for idx in range(len(tokens_2) - 2)])\n",
    "        else:\n",
    "            print \"Not correct n_gram parameter\"\n",
    "            return None\n",
    "\n",
    "    elif gram_type == 'symbol':\n",
    "        text_1 = \" \".join(forward_index[id_1])\n",
    "        text_2 = \" \".join(forward_index[id_2])\n",
    "        if n_gram == 2:\n",
    "            set_1 = set([\"\".join([text_1[idx], text_1[idx + 1]]) for idx in range(len(text_1) - 1)])\n",
    "            set_2 = set([\"\".join([text_2[idx], text_2[idx + 1]]) for idx in range(len(text_2) - 1)])\n",
    "        elif n_gram == 3:\n",
    "            set_1 = set([\"\".join([text_1[idx], text_1[idx + 1], text_1[idx + 2]])\\\n",
    "                         for idx in range(len(text_1) - 2)])\n",
    "            set_2 = set([\"\".join([text_2[idx], text_2[idx + 1], text_2[idx + 2]])\\\n",
    "                         for idx in range(len(text_2) - 2)])\n",
    "        else:\n",
    "            print \"Not correct n_gram parameter\"\n",
    "            return None\n",
    "    else:\n",
    "        print \"Not correct gram_type parameter\"\n",
    "        return None    \n",
    "\n",
    "    if feat_num == 1:\n",
    "        feature = len(set_1.intersection(set_2)) / float(len(set_1.union(set_2))) \\\n",
    "                                                    if len(set_1.union(set_2)) != 0 else 0\n",
    "    elif feat_num == 2:\n",
    "        feature = len(set_1.intersection(set_2)) / float(len(set_1)) if len(set_1) != 0 else 0\n",
    "    elif feat_num == 3:\n",
    "        feature = len(set_1.intersection(set_2)) / float(len(set_2)) if len(set_2) != 0 else 0\n",
    "    else:\n",
    "        print \"Not correct feat_num parameter\"\n",
    "        return None\n",
    "    \n",
    "    return feature\n",
    "\n",
    "#подсчет string - based features для всего датасета перифраз *создание numpy array*\n",
    "#всего 15 фичей: 9 для словарных N-Gram(1, 2, 3 слов), 6 для символьных(2 и 3 символов)\n",
    "def get_string_feature_for_all(forward_index, paraphrases):\n",
    "    all_string_features = []\n",
    "    for paraphrase in paraphrases:\n",
    "#         features = [paraphrase[0]]\n",
    "#         print \"PAIR_ID\", paraphrase[0]\n",
    "#         bprint(forward_index[paraphrase[1]])\n",
    "#         bprint(forward_index[paraphrase[2]])\n",
    "        features = []\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=1, gram_type='word', feat_num=1))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=2, gram_type='word', feat_num=1))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=3, gram_type='word', feat_num=1))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=1, gram_type='word', feat_num=2))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=2, gram_type='word', feat_num=2))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=3, gram_type='word', feat_num=2))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=1, gram_type='word', feat_num=3))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=2, gram_type='word', feat_num=3))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=3, gram_type='word', feat_num=3))\n",
    "        \n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=2, gram_type='symbol', feat_num=1))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=3, gram_type='symbol', feat_num=1))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=2, gram_type='symbol', feat_num=2))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=3, gram_type='symbol', feat_num=2))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=2, gram_type='symbol', feat_num=3))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=3, gram_type='symbol', feat_num=3))\n",
    "        all_string_features.append(features)\n",
    "    return np.asarray(all_string_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "полицейский\n",
      "разрешить\n",
      "стрелять\n",
      "на\n",
      "поражение\n",
      "по\n",
      "гражданин\n",
      "с\n",
      "травматика\n",
      "\n",
      "полиция\n",
      "мочь\n",
      "разрешить\n",
      "стрелять\n",
      "по\n",
      "хулиган\n",
      "с\n",
      "травматика\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4146341463414634"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bprint(forward_index[np_paraphrases[0, 1]])\n",
    "print \n",
    "bprint(forward_index[np_paraphrases[0, 2]])\n",
    "get_string_feature(forward_index, [np_paraphrases[0, 1], np_paraphrases[0, 2]], n_gram=3, gram_type='symbol', feat_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "string_features = get_string_feature_for_all(forward_index, np_paraphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7227, 15)\n"
     ]
    }
   ],
   "source": [
    "print np.asarray(string_features).shape\n",
    "with open(\"./string_features\", 'wb') as res_file:\n",
    "    pickle.dump(string_features, res_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## IR features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "    N - количество предложений \n",
    "    N(w_i) - количество предложений в которых встречается терм\n",
    "    avg - средняя длина предложения в датасете"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![Image](./img2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#количество предложений\n",
    "N = len(forward_index.keys())\n",
    "\n",
    "avg = 0\n",
    "for sent in forward_index.keys():\n",
    "    avg += len(forward_index[sent])\n",
    "    \n",
    "#средняя длина датасета\n",
    "avg /= float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_TF(token, sent_tokens):\n",
    "    tf = 0\n",
    "    for t in sent_tokens:\n",
    "        if t == token:\n",
    "            tf += 1\n",
    "    return tf\n",
    "\n",
    "def create_IDF(inverted_index):\n",
    "    # IDF = {токен: его IDF}\n",
    "    IDF = {}\n",
    "    for token in inverted_index.keys():\n",
    "        IDF[token] = np.log((N - len(inverted_index[token]) + 0.5) / float(len(inverted_index[token]) + 0.5))\n",
    "    return IDF\n",
    "        \n",
    "def BM25(id_1, id_2, forward_index, IDF, k = 1.2, b = 0.75):\n",
    "    tokens_1 = forward_index[id_1]\n",
    "    tokens_2 = forward_index[id_2]\n",
    "#     bprint(tokens_1)\n",
    "#     print\n",
    "#     bprint(tokens_2)\n",
    "    bm25 = 0\n",
    "    for token in tokens_1:\n",
    "        tf = get_TF(token, tokens_2)\n",
    "        tmp = (tf * (k + 1)) / float(tf + k * (1 - b + b * len(tokens_2)/avg))\n",
    "        bm25 += IDF[token] * tmp\n",
    "    return bm25\n",
    "\n",
    "#максимальный IDF слов, которыми различаются 2 предложения\n",
    "def maxIDF(id_1, id_2, forward_index, IDF):\n",
    "    tokens_1 = set(forward_index[id_1])\n",
    "    tokens_2 = set(forward_index[id_2])\n",
    "    difference = list(tokens_1.symmetric_difference(tokens_2))\n",
    "    if len(difference) == 0:\n",
    "        return 0\n",
    "    difference_IDF = [IDF[token] for token in difference]\n",
    "    return max(difference_IDF)\n",
    "\n",
    "#сумма IDF слов, которыми различаются 2 предложения\n",
    "def sumIDF(id_1, id_2, forward_index, IDF):\n",
    "    tokens_1 = set(forward_index[id_1])\n",
    "    tokens_2 = set(forward_index[id_2])\n",
    "    difference = list(tokens_1.symmetric_difference(tokens_2))\n",
    "    difference_IDF = [IDF[token] for token in difference]\n",
    "    return sum(difference_IDF)\n",
    "\n",
    "#формирует список всех IR фичей - BM25, maxIDF, sumIDF\n",
    "def get_IR_features_all(forward_index, inverted_index, paraphrases):\n",
    "    all_string_features = []\n",
    "    IDF = create_IDF(inverted_index)\n",
    "#     print IDF\n",
    "    for paraphrase in paraphrases:\n",
    "        id_1 = paraphrase[1]\n",
    "        id_2 = paraphrase[2]\n",
    "        features = []\n",
    "        features.append(BM25(id_1, id_2, forward_index, IDF))\n",
    "        features.append(maxIDF(id_1, id_2, forward_index, IDF))\n",
    "        features.append(sumIDF(id_1, id_2, forward_index, IDF))\n",
    "        all_string_features.append(features)\n",
    "    return np.asarray(all_string_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "IR_features = get_IR_features_all(forward_index, inverted_index, np_paraphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 25.51432783   7.38237325  34.89266782]\n",
      " [ 22.21936364   8.99230873  42.4968678 ]\n",
      " [ 23.43991854   8.4814002   34.48990074]\n",
      " [ 12.27017085   8.99230873  60.84430134]\n",
      " [ 21.92113702   6.87113279  36.32048699]]\n"
     ]
    }
   ],
   "source": [
    "print IR_features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.96857900846\n"
     ]
    }
   ],
   "source": [
    "print avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#выгрузка данных\n",
    "with open(\"./forward_index\", 'r') as res_file:\n",
    "    forward_index = pickle.load(res_file)\n",
    "with open(\"./inverted_index\", 'r') as res_file:\n",
    "    inverted_index = pickle.load(res_file)\n",
    "with open(\"./string_features\", 'r') as res_file:\n",
    "    string_features = pickle.load(res_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_test_data(class_cnt = 2):\n",
    "    tree = etree.parse('paraphrases_gold.xml')  \n",
    "    root = tree.getroot() \n",
    "    corpus = root[0]\n",
    "    data = []\n",
    "\n",
    "    for paraphrase in corpus:\n",
    "        new_pair_data = []\n",
    "        for field in paraphrase:\n",
    "            new_pair_data.append(field.text.encode('utf-8'))\n",
    "        data.append(new_pair_data)\n",
    "\n",
    "    Paraphrases = pd.DataFrame(np.asarray(data), columns = ['pair_id', 'id_1', 'id_2', 'text_1', 'text_2', 'class'])\n",
    "    Paraphrases[['pair_id', 'id_1', 'id_2', 'class']] = \\\n",
    "            Paraphrases[['pair_id', 'id_1', 'id_2', 'class']].apply(pd.to_numeric)\n",
    "    if class_cnt == 2:\n",
    "        Paraphrases['class'] = Paraphrases['class'].apply(lambda x: 1 if x >=0 else 0)\n",
    "    Paraphrases.to_csv(\"Paraphrases_test.csv\", index=False, encoding='utf-8')\n",
    "    return Paraphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np_test_data = np.asarray(get_test_data())\n",
    "# np_test_sentences = np_test_data[:,[1, 2]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#обратный индекс для тестовых данных: {токен:список id предложений,в которых токен встречается}\n",
    "test_inverted_index = {}\n",
    "#прямой индекс для тестовых данных: {id предложения:список токенов}\n",
    "test_forward_index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for pair in np_test_data:\n",
    "    tokens = re.findall('[\\w]+',pair[3].decode(\"utf-8\").strip().lower(), re.U)\n",
    "    tokens = [RusLem.parse(token)[0].normal_form for token in tokens]\n",
    "    test_forward_index[pair[1]] = tokens\n",
    "    tokens = re.findall('[\\w]+',pair[4].decode(\"utf-8\").strip().lower(), re.U)\n",
    "    tokens = [RusLem.parse(token)[0].normal_form for token in tokens]\n",
    "    test_forward_index[pair[2]] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_inverted_index = create_inverted_index(test_forward_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_string_features = get_string_feature_for_all(test_forward_index, np_test_data[:, :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_IR_features = get_IR_features_all(test_forward_index, test_inverted_index, np_test_data[:, :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_all_features = np.concatenate((test_string_features, test_IR_features), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1924, 18)\n",
      "(1924, 15)\n",
      "(1924, 3)\n"
     ]
    }
   ],
   "source": [
    "print test_all_features.shape\n",
    "print test_string_features.shape\n",
    "print test_IR_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Тrain Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Only String Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "target = np.asarray(Paraphrases[['class']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7227, 15)\n",
      "(7227, 1)\n"
     ]
    }
   ],
   "source": [
    "print string_features.shape\n",
    "print target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "string_model = GradientBoostingClassifier(loss = 'deviance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "parametrs_grid = {'learning_rate':[0.001,0.005, 0.01, 0.05, 0.1], 'n_estimators': [100, 300, 600, 800, 1000, 1200, 1400],\\\n",
    "                 'subsample':[0.6, 0.7, 0.8, 1.], 'max_depth':[2, 3, 4, 5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 560 candidates, totalling 3920 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:   19.1s\n",
      "[Parallel(n_jobs=10)]: Done 180 tasks      | elapsed:  8.4min\n",
      "[Parallel(n_jobs=10)]: Done 430 tasks      | elapsed: 23.8min\n",
      "[Parallel(n_jobs=10)]: Done 780 tasks      | elapsed: 68.3min\n",
      "[Parallel(n_jobs=10)]: Done 1230 tasks      | elapsed: 93.2min\n",
      "[Parallel(n_jobs=10)]: Done 1780 tasks      | elapsed: 144.6min\n",
      "[Parallel(n_jobs=10)]: Done 2430 tasks      | elapsed: 201.9min\n",
      "[Parallel(n_jobs=10)]: Done 3180 tasks      | elapsed: 266.1min\n",
      "[Parallel(n_jobs=10)]: Done 3920 out of 3920 | elapsed: 329.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=7, error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=100, presort='auto', random_state=None,\n",
       "              subsample=1.0, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=10,\n",
       "       param_grid={'n_estimators': [100, 300, 600, 800, 1000, 1200, 1400], 'subsample': [0.6, 0.7, 0.8, 1.0], 'learning_rate': [0.001, 0.005, 0.01, 0.05, 0.1], 'max_depth': [2, 3, 4, 5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=make_scorer(f1_score), verbose=1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_metric = make_scorer(f1_score)\n",
    "grid_model = GridSearchCV(string_model, parametrs_grid, cv = 7, verbose = 1, scoring=f1_metric, n_jobs=10)\n",
    "grid_model.fit(string_features, target.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 100, 'subsample': 0.6}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.01, loss='deviance', max_depth=4,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=100, presort='auto', random_state=None,\n",
       "              subsample=0.6, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_GB = GradientBoostingClassifier(loss = 'deviance', learning_rate=0.01, \\\n",
    "                                     max_depth=4, n_estimators=100, subsample=0.6)\n",
    "best_GB.fit(string_features, target.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST Gradient Boosting Result on String Features\n",
      "F1-score 0.789415656009\n",
      "Accuracy: 0.702182952183\n"
     ]
    }
   ],
   "source": [
    "test_predict = best_GB.predict(test_string_features)\n",
    "print \"BEST Gradient Boosting Result on String Features\"\n",
    "print \"F1-score\", f1_score(np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict)\n",
    "print \"Accuracy:\", accuracy_score(np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict)\n",
    "# print (classification_report( np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict, target_names= ['non-paraphrases', 'paraphrases']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78941565600882035"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with openen(\"string_model\", \"wb\") as res_file:\n",
    "    pickle.dump(best_GBres_files_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7227, 3)\n",
      "(7227, 1)\n"
     ]
    }
   ],
   "source": [
    "target = np.asarray(Paraphrases[['class']])\n",
    "print IR_features.shape\n",
    "print target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Only IR features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "IR_model = GradientBoostingClassifier(loss = 'deviance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "parametrs_grid = {'learning_rate':[0.005, 0.01, 0.1], 'n_estimators': [100, 300, 500, 700, 1000, 1300],\\\n",
    "                 'subsample':[0.6, 0.8, 1.], 'max_depth':[2, 3, 4, 5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 216 candidates, totalling 1512 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:   16.6s\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed: 12.1min\n",
      "[Parallel(n_jobs=5)]: Done 790 tasks      | elapsed: 23.1min\n",
      "[Parallel(n_jobs=5)]: Done 1240 tasks      | elapsed: 38.0min\n",
      "[Parallel(n_jobs=5)]: Done 1512 out of 1512 | elapsed: 48.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=7, error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=100, presort='auto', random_state=None,\n",
       "              subsample=1.0, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=5,\n",
       "       param_grid={'n_estimators': [100, 300, 500, 700, 1000, 1300], 'subsample': [0.6, 0.8, 1.0], 'learning_rate': [0.005, 0.01, 0.1], 'max_depth': [2, 3, 4, 5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=make_scorer(f1_score), verbose=1)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_metric = make_scorer(f1_score)\n",
    "grid_model = GridSearchCV(IR_model, parametrs_grid, cv = 7, verbose = 1, scoring=f1_metric, n_jobs=5)\n",
    "grid_model.fit(IR_features, target.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST Gradient Boosting Result on IR Features\n",
      "\n",
      "F1-score 0.779085872576\n",
      "Accuracy: 0.668399168399\n"
     ]
    }
   ],
   "source": [
    "best_GB_IR = GradientBoostingClassifier(loss = 'deviance', learning_rate=0.01, \\\n",
    "                                     max_depth=3, n_estimators=100, subsample=1.0)\n",
    "best_GB_IR.fit(IR_features, target.ravel())\n",
    "\n",
    "test_predict = best_GB_IR.predict(test_IR_features)\n",
    "print \"BEST Gradient Boosting Result on IR Features\"\n",
    "print \n",
    "print \"F1-score\", f1_score(np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict)\n",
    "print \"Accuracy:\", accuracy_score(np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict)\n",
    "# print (classification_report( np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict, target_names= ['non-paraphrases', 'paraphrases']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### String + IR features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7227, 18)\n"
     ]
    }
   ],
   "source": [
    "all_features = np.concatenate((string_features, IR_features), axis = 1)\n",
    "print all_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "string_IR_model = GradientBoostingClassifier(loss = 'deviance')\n",
    "parametrs_grid = {'learning_rate':[0.005, 0.01, 0.05], 'n_estimators': [100, 300, 500, 700, 1000, 1300],\\\n",
    "                 'subsample':[0.6, 0.7, 0.8, 1.], 'max_depth':[2, 3, 4, 5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 288 candidates, totalling 2016 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:   33.1s\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed: 27.0min\n",
      "[Parallel(n_jobs=5)]: Done 790 tasks      | elapsed: 64.1min\n",
      "[Parallel(n_jobs=5)]: Done 1240 tasks      | elapsed: 97.8min\n",
      "[Parallel(n_jobs=5)]: Done 1790 tasks      | elapsed: 140.9min\n",
      "[Parallel(n_jobs=5)]: Done 2016 out of 2016 | elapsed: 170.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=7, error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=100, presort='auto', random_state=None,\n",
       "              subsample=1.0, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=5,\n",
       "       param_grid={'n_estimators': [100, 300, 500, 700, 1000, 1300], 'subsample': [0.6, 0.7, 0.8, 1.0], 'learning_rate': [0.005, 0.01, 0.05], 'max_depth': [2, 3, 4, 5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=make_scorer(f1_score), verbose=1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_metric = make_scorer(f1_score)\n",
    "grid_model = GridSearchCV(string_IR_model, parametrs_grid, cv = 7, verbose = 1, n_jobs=5, scoring=f1_metric)\n",
    "grid_model.fit(all_features, target.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.005, 'max_depth': 2, 'n_estimators': 500, 'subsample': 0.7}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST Gradient Boosting Result on String+IR Features\n",
      "\n",
      "F1-score 0.80221402214\n",
      "Accuracy: 0.721413721414\n"
     ]
    }
   ],
   "source": [
    "best_GB_IR_string = GradientBoostingClassifier(loss = 'deviance', learning_rate=0.05, \\\n",
    "                                     max_depth=2, n_estimators=500, subsample=0.7)\n",
    "best_GB_IR_string.fit(all_features, target.ravel())\n",
    "\n",
    "test_predict = best_GB_IR_string.predict(test_all_features)\n",
    "print \"BEST Gradient Boosting Result on String+IR Features\"\n",
    "print \n",
    "print \"F1-score\", f1_score(np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict)\n",
    "print \"Accuracy:\", accuracy_score(np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Experiments with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291584\r\n"
     ]
    }
   ],
   "source": [
    "! ls -1 ../news | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RusLem = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#формирование датасета для W2V из тренировочной выборки\n",
    "sentences_text = np.asarray(Sentences[['text']])\n",
    "print bprint(sentences_text[0])\n",
    "print len(sentences_text)\n",
    "sentences = []\n",
    "\n",
    "for sent in sentences_text:\n",
    "    tokens = re.findall('[\\w]+',sent[0].decode(\"utf-8\").strip().lower(), re.U)\n",
    "    tokens = [RusLem.parse(token)[0].normal_form for token in tokens]\n",
    "    sentences.append(tokens)\n",
    "    \n",
    "print bprint(sentences[0])\n",
    "print len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare 0 file\n",
      "Prepare 10000 file\n",
      "Prepare 20000 file\n",
      "Prepare 30000 file\n",
      "Prepare 40000 file\n",
      "Prepare 50000 file\n",
      "Prepare 60000 file\n",
      "Prepare 70000 file\n",
      "Prepare 80000 file\n",
      "Prepare 90000 file\n",
      "Prepare 100000 file\n",
      "Prepare 110000 file\n",
      "Prepare 120000 file\n",
      "Prepare 130000 file\n",
      "Prepare 140000 file\n",
      "Prepare 150000 file\n",
      "Prepare 160000 file\n",
      "Prepare 170000 file\n",
      "Prepare 180000 file\n",
      "Prepare 190000 file\n",
      "Prepare 200000 file\n",
      "Prepare 210000 file\n",
      "Prepare 220000 file\n"
     ]
    }
   ],
   "source": [
    "#предобработка датасета новостных статей\n",
    "news_files = os.listdir(\"../news\")\n",
    "# news_text = \"\"\n",
    "with io.open(\"news_text\", 'w', encoding='utf-8') as news_text_file:\n",
    "    for idx, f in enumerate(news_files):\n",
    "        if idx % 10000 == 0:\n",
    "            print \"Prepare\", idx, 'file'\n",
    "        with open(\"../news/\" + f, 'r') as news_file:\n",
    "            js = json.load(news_file, encoding='utf-8')\n",
    "            news_text_file.write(js['text'] +  \\\n",
    "                                 js['title'] + '\\n')\n",
    "\n",
    "print json.load(file(\"../news/\" + 'news_0246791.json', 'r'))['text']\n",
    "print \n",
    "print json.load(file(\"../news/\" + 'news_0246791.json', 'r'))['title']\n",
    "json.load(file(\"../news/\" + 'news_0246791.json', 'r')).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wc -l news_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#формирование данных для обучения Word2Vec из датасета новостных статей на русском\n",
    "new_sentences = []\n",
    "with open(\"./news_text\", \"r\") as news_text:\n",
    "    for ind, line in enumerate(news_text):\n",
    "        if ind % 10000 == 0:\n",
    "            print \"Prepare\", ind, \"line\"\n",
    "        for sent in line.strip().split('.'):\n",
    "#             print sent\n",
    "            tokens = re.findall('[\\w]+',sent.decode(\"utf-8\").strip().lower(), re.U)\n",
    "            tokens = [RusLem.parse(token)[0].normal_form for token in tokens]\n",
    "            new_sentences.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#объединение тренировочного датасета и датасета новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#обучение Word2Vec\n",
    "model = Word2Vec(size=200, window=7, min_count=0)\n",
    "model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#создание прямого индекса из embedding слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможные признаки:\n",
    "    1. Усреднение векторов всех слов\n",
    "    2. Максимальная близость между двумя словами в предложении - ?? \n",
    "    3. Максимальная близость между словами, которыми отличаются 2 предложения\n",
    "    4. Взвешенная сумма векторов слов: умноженное на IDF слова\n",
    "    3. Попарная близость все слов 2-х предложений -> N * N фичей, где N - максимальное количество слов в предложении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
