{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraphrase Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lxml import etree       \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import chardet\n",
    "import pymorphy2\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#необходимо для красивой печать токенов на русском\n",
    "def bprint(x):\n",
    "    for x_ in x:\n",
    "        print x_\n",
    "#     print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The creating of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#формирую dataframe из предложений - информация о парафразах\n",
    "#Paraphrase classes: -1: non-paraphrases, 0: loose paraphrases, 1: strict paraphrases.\n",
    "tree = etree.parse('paraphrases.xml')  \n",
    "root = tree.getroot() \n",
    "corpus = root[1]\n",
    "data = []\n",
    "\n",
    "for paraphrase in corpus:\n",
    "    new_pair_data = []\n",
    "    for field in paraphrase:\n",
    "        new_pair_data.append(field.text.encode('utf-8'))\n",
    "    data.append(new_pair_data)\n",
    "\n",
    "Paraphrases = pd.DataFrame(np.asarray(data), columns = ['pair_id', 'id_1', 'id_2', 'text_1', 'text_2', 'jaccard', 'class'])\n",
    "Paraphrases[['pair_id', 'id_1', 'id_2', 'jaccard', 'class']] = \\\n",
    "        Paraphrases[['pair_id', 'id_1', 'id_2', 'jaccard', 'class']].apply(pd.to_numeric)\n",
    "Paraphrases.to_csv(\"Paraphrases.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#формирую dataframe для предложений - информация о предложениях\n",
    "#Paraphrase classes: -1: non-paraphrases, 0: loose paraphrases, 1: strict paraphrases.\n",
    "tree = etree.parse('corpus.xml')  \n",
    "root = tree.getroot() \n",
    "corpus = root[1]\n",
    "data = []\n",
    "\n",
    "for sentense in corpus:\n",
    "    new_sentense = []\n",
    "    for field in sentense:\n",
    "        if field.text is None:\n",
    "            new_sentense.append(None)\n",
    "        else:\n",
    "            new_sentense.append(field.text.encode('utf-8'))\n",
    "    data.append(new_sentense)\n",
    "\n",
    "Sentences = pd.DataFrame(np.asarray(data), columns = ['id', 'text', 'agency', 'author', 'url', 'date'])\n",
    "Sentences[['id']] = Sentences[['id']].apply(pd.to_numeric) \n",
    "Sentences.to_csv(\"Sentences.csv\",index = False,  encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (7227, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_id</th>\n",
       "      <th>id_1</th>\n",
       "      <th>id_2</th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>jaccard</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>201</td>\n",
       "      <td>8159</td>\n",
       "      <td>Полицейским разрешат стрелять на поражение по ...</td>\n",
       "      <td>Полиции могут разрешить стрелять по хулиганам ...</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>202</td>\n",
       "      <td>8158</td>\n",
       "      <td>Право полицейских на проникновение в жилище ре...</td>\n",
       "      <td>Правила внесудебного проникновения полицейских...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>273</td>\n",
       "      <td>8167</td>\n",
       "      <td>Президент Египта ввел чрезвычайное положение в...</td>\n",
       "      <td>Власти Египта угрожают ввести в стране чрезвыч...</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>220</td>\n",
       "      <td>8160</td>\n",
       "      <td>Вернувшихся из Сирии россиян волнует вопрос тр...</td>\n",
       "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
       "      <td>0.324037</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>8160</td>\n",
       "      <td>В Москву из Сирии вернулись 2 самолета МЧС с р...</td>\n",
       "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
       "      <td>0.606218</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pair_id  id_1  id_2                                             text_1  \\\n",
       "0        1   201  8159  Полицейским разрешат стрелять на поражение по ...   \n",
       "1        2   202  8158  Право полицейских на проникновение в жилище ре...   \n",
       "2        3   273  8167  Президент Египта ввел чрезвычайное положение в...   \n",
       "3        4   220  8160  Вернувшихся из Сирии россиян волнует вопрос тр...   \n",
       "4        5   223  8160  В Москву из Сирии вернулись 2 самолета МЧС с р...   \n",
       "\n",
       "                                              text_2   jaccard  class  \n",
       "0  Полиции могут разрешить стрелять по хулиганам ...  0.650000      0  \n",
       "1  Правила внесудебного проникновения полицейских...  0.500000      0  \n",
       "2  Власти Египта угрожают ввести в стране чрезвыч...  0.611429      0  \n",
       "3  Самолеты МЧС вывезут россиян из разрушенной Си...  0.324037     -1  \n",
       "4  Самолеты МЧС вывезут россиян из разрушенной Си...  0.606218      0  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"Train data shape: \", Paraphrases.shape\n",
    "Paraphrases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences number:  (12062, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>agency</th>\n",
       "      <th>author</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Избежать \"фискального обрыва\": Сенат США подде...</td>\n",
       "      <td>РБК</td>\n",
       "      <td>None</td>\n",
       "      <td>http://top.rbc.ru/economics/01/01/2013/839229....</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>\"Фискальный обрыв\" в США временно предотвращен.</td>\n",
       "      <td>РБК</td>\n",
       "      <td>None</td>\n",
       "      <td>http://top.rbc.ru/economics/01/01/2013/839223....</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Чечня попросила националистов составить кодекс...</td>\n",
       "      <td>РБК</td>\n",
       "      <td>None</td>\n",
       "      <td>http://top.rbc.ru/society/01/01/2013/839242.shtml</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Северокорейский лидер впервые за 19 лет поздра...</td>\n",
       "      <td>РБК</td>\n",
       "      <td>None</td>\n",
       "      <td>http://top.rbc.ru/society/01/01/2013/839227.shtml</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>В Кот-Д`Ивуаре десятки человек погибли в давке...</td>\n",
       "      <td>РБК</td>\n",
       "      <td>None</td>\n",
       "      <td>http://top.rbc.ru/incidents/01/01/2013/839240....</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text agency author  \\\n",
       "0   1  Избежать \"фискального обрыва\": Сенат США подде...    РБК   None   \n",
       "1   2    \"Фискальный обрыв\" в США временно предотвращен.    РБК   None   \n",
       "2   3  Чечня попросила националистов составить кодекс...    РБК   None   \n",
       "3   4  Северокорейский лидер впервые за 19 лет поздра...    РБК   None   \n",
       "4   5  В Кот-Д`Ивуаре десятки человек погибли в давке...    РБК   None   \n",
       "\n",
       "                                                 url        date  \n",
       "0  http://top.rbc.ru/economics/01/01/2013/839229....  2013-01-01  \n",
       "1  http://top.rbc.ru/economics/01/01/2013/839223....  2013-01-01  \n",
       "2  http://top.rbc.ru/society/01/01/2013/839242.shtml  2013-01-01  \n",
       "3  http://top.rbc.ru/society/01/01/2013/839227.shtml  2013-01-01  \n",
       "4  http://top.rbc.ru/incidents/01/01/2013/839240....  2013-01-01  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"Sentences number: \", Sentences.shape\n",
    "Sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                               text agency author  \\\n",
      "0   1  Избежать \"фискального обрыва\": Сенат США подде...    РБК    NaN   \n",
      "1   2    \"Фискальный обрыв\" в США временно предотвращен.    РБК    NaN   \n",
      "2   3  Чечня попросила националистов составить кодекс...    РБК    NaN   \n",
      "3   4  Северокорейский лидер впервые за 19 лет поздра...    РБК    NaN   \n",
      "4   5  В Кот-Д`Ивуаре десятки человек погибли в давке...    РБК    NaN   \n",
      "\n",
      "                                                 url        date  \n",
      "0  http://top.rbc.ru/economics/01/01/2013/839229....  2013-01-01  \n",
      "1  http://top.rbc.ru/economics/01/01/2013/839223....  2013-01-01  \n",
      "2  http://top.rbc.ru/society/01/01/2013/839242.shtml  2013-01-01  \n",
      "3  http://top.rbc.ru/society/01/01/2013/839227.shtml  2013-01-01  \n",
      "4  http://top.rbc.ru/incidents/01/01/2013/839240....  2013-01-01  \n",
      "   pair_id  id_1  id_2                                             text_1  \\\n",
      "0        1   201  8159  Полицейским разрешат стрелять на поражение по ...   \n",
      "1        2   202  8158  Право полицейских на проникновение в жилище ре...   \n",
      "2        3   273  8167  Президент Египта ввел чрезвычайное положение в...   \n",
      "3        4   220  8160  Вернувшихся из Сирии россиян волнует вопрос тр...   \n",
      "4        5   223  8160  В Москву из Сирии вернулись 2 самолета МЧС с р...   \n",
      "\n",
      "                                              text_2   jaccard  class  \n",
      "0  Полиции могут разрешить стрелять по хулиганам ...  0.650000      0  \n",
      "1  Правила внесудебного проникновения полицейских...  0.500000      0  \n",
      "2  Власти Египта угрожают ввести в стране чрезвыч...  0.611429      0  \n",
      "3  Самолеты МЧС вывезут россиян из разрушенной Си...  0.324037     -1  \n",
      "4  Самолеты МЧС вывезут россиян из разрушенной Си...  0.606218      0  \n"
     ]
    }
   ],
   "source": [
    "Paraphrases = pd.read_csv(\"Paraphrases.csv\")\n",
    "print Paraphrases.head()\n",
    "Sentences = pd.read_csv(\"Sentences.csv\")\n",
    "print Sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lemmatization and constructing the inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RusLem = pymorphy2.MorphAnalyzer()\n",
    "#обратный индес: {токен:список id предложений,в которых токен встречается}\n",
    "inverted_index = {}\n",
    "#прямой индекс: {id предложения:список токенов}\n",
    "forward_index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np_sentences = np.asarray(Sentences[['id', 'text']])\n",
    "\n",
    "for sent in np_sentences:\n",
    "    tokens = re.findall('[\\w]+',sent[1].decode(\"utf-8\").strip().lower(), re.U)\n",
    "    tokens = [RusLem.parse(token)[0].normal_form for token in tokens]\n",
    "    forward_index[sent[0]] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_inverted_index(forward_index):\n",
    "    inverted_index = {}\n",
    "    for sent_id in forward_index.keys():\n",
    "        for token in forward_index[sent_id]:\n",
    "            if token not in inverted_index:\n",
    "                inverted_index[token] = [sent_id]\n",
    "            elif sent_id not in inverted_index[token]:\n",
    "                inverted_index[token].append(sent_id)\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# bprint(np_sentences[:5, 1])\n",
    "inverted_index = create_inverted_index(forward_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"./forward_index\", 'wb') as res_file:\n",
    "    pickle.dump(forward_index, res_file)\n",
    "with open(\"./inverted_index\", 'wb') as res_file:\n",
    "    pickle.dump(inverted_index, res_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String - based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25495 34424 34611]\n",
      " [25496 34596 34611]\n",
      " [25498 34488 34613]\n",
      " [25499 34614 34615]\n",
      " [25500 34616 34617]\n",
      " [25514 34622 34633]\n",
      " [25524 34566 34654]\n",
      " [25548 34519 34681]\n",
      " [25549 34565 34681]\n",
      " [25577 34584 34722]]\n"
     ]
    }
   ],
   "source": [
    "np_paraphrases = np.asarray(Paraphrases[['pair_id', 'id_1', 'id_2']])\n",
    "print np_paraphrases[len(np_paraphrases) - 10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](img1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#возвращает значение признака для 1 пары предложений\n",
    "#paraphrase = [id_1, id_2]\n",
    "def get_string_feature(forward_index, paraphrase, n_gram = 1, gram_type = 'word', feat_num = 1):\n",
    "    id_1 = paraphrase[0]\n",
    "    id_2 = paraphrase[1]\n",
    "#     bprint(forward_index[id_1])\n",
    "#     bprint(forward_index[id_2])\n",
    "    if gram_type == 'word':\n",
    "        tokens_1 = forward_index[id_1]\n",
    "        tokens_2 = forward_index[id_2]\n",
    "        if n_gram == 1:\n",
    "            set_1 = set(tokens_1)\n",
    "            set_2 = set(tokens_2)\n",
    "#             print len(set_1)\n",
    "#             bprint(list(set_1))\n",
    "#             print len(set_2)\n",
    "#             bprint(list(set_2))\n",
    "#             print len(set_1.intersection(set_2))\n",
    "#             bprint(set_1.intersection(set_2))\n",
    "        elif n_gram == 2:\n",
    "            set_1 = set([\" \".join([tokens_1[idx], tokens_1[idx + 1]]) for idx in range(len(tokens_1) - 1)])\n",
    "            set_2 = set([\" \".join([tokens_2[idx], tokens_2[idx + 1]]) for idx in range(len(tokens_2) - 1)])\n",
    "#             print len(set_1)\n",
    "#             bprint(list(set_1))\n",
    "#             print len(set_2)\n",
    "#             bprint(list(set_2))\n",
    "#             print len(set_1.intersection(set_2))\n",
    "#             bprint(set_1.intersection(set_2))\n",
    "        elif n_gram == 3:\n",
    "            set_1 = set([\" \".join([tokens_1[idx], tokens_1[idx + 1], tokens_1[idx + 2]]) \\\n",
    "                                                     for idx in range(len(tokens_1) - 2)])\n",
    "            set_2 = set([\" \".join([tokens_2[idx], tokens_2[idx + 1], tokens_2[idx + 2]])\\\n",
    "                                                     for idx in range(len(tokens_2) - 2)])\n",
    "#             print len(set_1)\n",
    "#             print \"------------------------------------\"\n",
    "#             bprint(list(set_1))\n",
    "#             print len(set_2)\n",
    "#             bprint(list(set_2))\n",
    "#             print len(set_1.intersection(set_2))\n",
    "#             bprint(set_1.intersection(set_2))\n",
    "        else:\n",
    "            print \"Not correct n_gram parameter\"\n",
    "            return None\n",
    "\n",
    "    elif gram_type == 'symbol':\n",
    "        text_1 = \" \".join(forward_index[id_1])\n",
    "        text_2 = \" \".join(forward_index[id_2])\n",
    "        if n_gram == 2:\n",
    "            set_1 = set([\"\".join([text_1[idx], text_1[idx + 1]]) for idx in range(len(text_1) - 1)])\n",
    "            set_2 = set([\"\".join([text_2[idx], text_2[idx + 1]]) for idx in range(len(text_2) - 1)])\n",
    "#             print len(set_1)\n",
    "#             bprint(list(set_1))\n",
    "#             print len(set_2)\n",
    "#             bprint(list(set_2))\n",
    "#             print len(set_1.intersection(set_2))\n",
    "        elif n_gram == 3:\n",
    "            set_1 = set([\"\".join([text_1[idx], text_1[idx + 1], text_1[idx + 2]])\\\n",
    "                         for idx in range(len(text_1) - 2)])\n",
    "            set_2 = set([\"\".join([text_2[idx], text_2[idx + 1], text_2[idx + 2]])\\\n",
    "                         for idx in range(len(text_2) - 2)])\n",
    "#             print len(set_1)\n",
    "#             bprint(list(set_1))\n",
    "#             print \"_________________________________\"\n",
    "#             print len(set_2)\n",
    "#             bprint(list(set_2))\n",
    "#             print len(set_1.intersection(set_2))\n",
    "        else:\n",
    "            print \"Not correct n_gram parameter\"\n",
    "            return None\n",
    "    else:\n",
    "        print \"Not correct gram_type parameter\"\n",
    "        return None    \n",
    "\n",
    "    if feat_num == 1:\n",
    "        feature = len(set_1.intersection(set_2)) / float(len(set_1.union(set_2))) \\\n",
    "                                                    if len(set_1.union(set_2)) != 0 else 0\n",
    "    elif feat_num == 2:\n",
    "        feature = len(set_1.intersection(set_2)) / float(len(set_1)) if len(set_1) != 0 else 0\n",
    "    elif feat_num == 3:\n",
    "#         bprint(text_2)\n",
    "#         bprint(set_2)\n",
    "#         print len(set_2)\n",
    "        feature = len(set_1.intersection(set_2)) / float(len(set_2)) if len(set_2) != 0 else 0\n",
    "    else:\n",
    "        print \"Not correct feat_num parameter\"\n",
    "        return None\n",
    "    \n",
    "    return feature\n",
    "\n",
    "#подсчет string - based features для всего датасета перифраз *создание numpy array*\n",
    "#всего 15 фичей: 9 для словарных N-Gram(1, 2, 3 слов), 6 для символьных(2 и 3 символов)\n",
    "def get_string_feature_for_all(forward_index, paraphrases):\n",
    "    all_string_features = []\n",
    "    for paraphrase in paraphrases:\n",
    "#         features = [paraphrase[0]]\n",
    "#         print \"PAIR_ID\", paraphrase[0]\n",
    "#         bprint(forward_index[paraphrase[1]])\n",
    "#         bprint(forward_index[paraphrase[2]])\n",
    "        features = []\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=1, gram_type='word', feat_num=1))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=2, gram_type='word', feat_num=1))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=3, gram_type='word', feat_num=1))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=1, gram_type='word', feat_num=2))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=2, gram_type='word', feat_num=2))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=3, gram_type='word', feat_num=2))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=1, gram_type='word', feat_num=3))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=2, gram_type='word', feat_num=3))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=3, gram_type='word', feat_num=3))\n",
    "        \n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=2, gram_type='symbol', feat_num=1))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=3, gram_type='symbol', feat_num=1))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=2, gram_type='symbol', feat_num=2))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=3, gram_type='symbol', feat_num=2))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=2, gram_type='symbol', feat_num=3))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=3, gram_type='symbol', feat_num=3))\n",
    "        all_string_features.append(features)\n",
    "    return all_string_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "полицейский\n",
      "разрешить\n",
      "стрелять\n",
      "на\n",
      "поражение\n",
      "по\n",
      "гражданин\n",
      "с\n",
      "травматика\n",
      "\n",
      "полиция\n",
      "мочь\n",
      "разрешить\n",
      "стрелять\n",
      "по\n",
      "хулиган\n",
      "с\n",
      "травматика\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4146341463414634"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bprint(forward_index[np_paraphrases[0, 1]])\n",
    "print \n",
    "bprint(forward_index[np_paraphrases[0, 2]])\n",
    "get_string_feature(forward_index, [np_paraphrases[0, 1], np_paraphrases[0, 2]], n_gram=3, gram_type='symbol', feat_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "string_features = get_string_feature_for_all(forward_index, np_paraphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7227, 15)\n"
     ]
    }
   ],
   "source": [
    "print np.asarray(string_features).shape\n",
    "with open(\"./string_features\", 'wb') as res_file:\n",
    "    pickle.dump(string_features, res_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IR features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](./img2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#количество предложений\n",
    "N = len(forward_index.keys())\n",
    "\n",
    "avg = 0\n",
    "for sent in forward_index.keys():\n",
    "    avg += len(forward_index[sent])\n",
    "#средняя длина датасета\n",
    "avg /= float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_TF(token, sent_tokens):\n",
    "    tf = 0\n",
    "    for t in sent_tokens:\n",
    "        if t == token:\n",
    "            tf += 1\n",
    "    return tf\n",
    "\n",
    "def create_IDF(inverse_index):\n",
    "    # IDF = {токен: его IDF}\n",
    "    IDF = {}\n",
    "    for token in inverse_index.keys():\n",
    "        IDF[token] = np.log((N - len(inverted_index[token]) + 0.5) / float(len(inverted_index[token]) + 0.5))\n",
    "        \n",
    "def BM25(id_1, id_2, forward_index, IDF, k = 1.2, b = 0.75):\n",
    "    tokens_1 = forward_index[id_1]\n",
    "    tokens_2 = forward_index[id_2]\n",
    "    bm25 = 0\n",
    "    for token in tokens_1:\n",
    "        tf = get_TF(token, tokens_2)\n",
    "        tmp = (tf * (k + 1)) / float(tf + k * (1 - b + b * len(tokens_2)/avg))\n",
    "        bm25 += IDF[token] * tmp\n",
    "\n",
    "#максимальный IDF слов, которыми различаются 2 предложения\n",
    "def maxIDF(id_1, id_2, forward_index):\n",
    "    tokens_1 = set(forward_index[id_1])\n",
    "    tokens_2 = set(forward_index[id_1])\n",
    "    difference = list(tokens_1.symmetric_difference(tokens_2))\n",
    "    difference_IDF = [IDF[token] for token in difference]\n",
    "    return max(difference_IDF)\n",
    "\n",
    "#сумма IDF слов, которыми различаются 2 предложения\n",
    "def sumIDF(id_1, id_2, forward_index):\n",
    "    tokens_1 = set(forward_index[id_1])\n",
    "    tokens_2 = set(forward_index[id_1])\n",
    "    difference = list(tokens_1.symmetric_difference(tokens_2))\n",
    "    difference_IDF = [IDF[token] for token in difference]\n",
    "    return sum(difference_IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
