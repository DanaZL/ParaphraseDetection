{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Paraphrase Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from lxml import etree       \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import chardet\n",
    "import pymorphy2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import re\n",
    "import cPickle as pickle\n",
    "import os\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "import json\n",
    "import io\n",
    "import scipy\n",
    "import sys\n",
    "import gc\n",
    "import random as r\n",
    "# import logging\n",
    "# reload(logging)\n",
    "# logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#необходимо для красивой печати токенов на русском\n",
    "def bprint(x):\n",
    "    for x_ in x:\n",
    "        print x_\n",
    "#     print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## The creation of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#формирую dataframe из предложений - информация о парафразах\n",
    "#Paraphrase classes: -1: non-paraphrases, 0: loose paraphrases, 1: strict paraphrases.\n",
    "##Для бинарной классификации: 0 - не парафразы, 1 - парафразы\n",
    "tree = etree.parse('paraphrases.xml')  \n",
    "root = tree.getroot() \n",
    "corpus = root[1]\n",
    "data = []\n",
    "\n",
    "for paraphrase in corpus:\n",
    "    new_pair_data = []\n",
    "    for field in paraphrase:\n",
    "        new_pair_data.append(field.text.encode('utf-8'))\n",
    "    data.append(new_pair_data)\n",
    "\n",
    "Paraphrases = pd.DataFrame(np.asarray(data), columns = ['pair_id', 'id_1', 'id_2', 'text_1', 'text_2', 'jaccard', 'class'])\n",
    "Paraphrases[['pair_id', 'id_1', 'id_2', 'jaccard', 'class']] = \\\n",
    "        Paraphrases[['pair_id', 'id_1', 'id_2', 'jaccard', 'class']].apply(pd.to_numeric)\n",
    "Paraphrases['class'] = Paraphrases['class'].apply(lambda x: 1 if x >=0 else 0)\n",
    "Paraphrases.to_csv(\"Paraphrases.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#формирую dataframe для предложений - информация о предложениях\n",
    "#Paraphrase classes: -1: non-paraphrases, 0: loose paraphrases, 1: strict paraphrases.\n",
    "tree = etree.parse('corpus.xml')  \n",
    "root = tree.getroot() \n",
    "corpus = root[1]\n",
    "data = []\n",
    "\n",
    "for sentense in corpus:\n",
    "    new_sentense = []\n",
    "    for field in sentense:\n",
    "        if field.text is None:\n",
    "            new_sentense.append(None)\n",
    "        else:\n",
    "            new_sentense.append(field.text.encode('utf-8'))\n",
    "    data.append(new_sentense)\n",
    "\n",
    "Sentences = pd.DataFrame(np.asarray(data), columns = ['id', 'text', 'agency', 'author', 'url', 'date'])\n",
    "Sentences[['id']] = Sentences[['id']].apply(pd.to_numeric) \n",
    "Sentences.to_csv(\"Sentences.csv\",index = False,  encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (7227, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_id</th>\n",
       "      <th>id_1</th>\n",
       "      <th>id_2</th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>jaccard</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>201</td>\n",
       "      <td>8159</td>\n",
       "      <td>Полицейским разрешат стрелять на поражение по ...</td>\n",
       "      <td>Полиции могут разрешить стрелять по хулиганам ...</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>202</td>\n",
       "      <td>8158</td>\n",
       "      <td>Право полицейских на проникновение в жилище ре...</td>\n",
       "      <td>Правила внесудебного проникновения полицейских...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>273</td>\n",
       "      <td>8167</td>\n",
       "      <td>Президент Египта ввел чрезвычайное положение в...</td>\n",
       "      <td>Власти Египта угрожают ввести в стране чрезвыч...</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>220</td>\n",
       "      <td>8160</td>\n",
       "      <td>Вернувшихся из Сирии россиян волнует вопрос тр...</td>\n",
       "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
       "      <td>0.324037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>8160</td>\n",
       "      <td>В Москву из Сирии вернулись 2 самолета МЧС с р...</td>\n",
       "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
       "      <td>0.606218</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pair_id  id_1  id_2                                             text_1  \\\n",
       "0        1   201  8159  Полицейским разрешат стрелять на поражение по ...   \n",
       "1        2   202  8158  Право полицейских на проникновение в жилище ре...   \n",
       "2        3   273  8167  Президент Египта ввел чрезвычайное положение в...   \n",
       "3        4   220  8160  Вернувшихся из Сирии россиян волнует вопрос тр...   \n",
       "4        5   223  8160  В Москву из Сирии вернулись 2 самолета МЧС с р...   \n",
       "\n",
       "                                              text_2   jaccard  class  \n",
       "0  Полиции могут разрешить стрелять по хулиганам ...  0.650000      1  \n",
       "1  Правила внесудебного проникновения полицейских...  0.500000      1  \n",
       "2  Власти Египта угрожают ввести в стране чрезвыч...  0.611429      1  \n",
       "3  Самолеты МЧС вывезут россиян из разрушенной Си...  0.324037      0  \n",
       "4  Самолеты МЧС вывезут россиян из разрушенной Си...  0.606218      1  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"Train data shape: \", Paraphrases.shape\n",
    "Paraphrases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences number:  (12062, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>agency</th>\n",
       "      <th>author</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Избежать \"фискального обрыва\": Сенат США подде...</td>\n",
       "      <td>РБК</td>\n",
       "      <td>None</td>\n",
       "      <td>http://top.rbc.ru/economics/01/01/2013/839229....</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>\"Фискальный обрыв\" в США временно предотвращен.</td>\n",
       "      <td>РБК</td>\n",
       "      <td>None</td>\n",
       "      <td>http://top.rbc.ru/economics/01/01/2013/839223....</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Чечня попросила националистов составить кодекс...</td>\n",
       "      <td>РБК</td>\n",
       "      <td>None</td>\n",
       "      <td>http://top.rbc.ru/society/01/01/2013/839242.shtml</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Северокорейский лидер впервые за 19 лет поздра...</td>\n",
       "      <td>РБК</td>\n",
       "      <td>None</td>\n",
       "      <td>http://top.rbc.ru/society/01/01/2013/839227.shtml</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>В Кот-Д`Ивуаре десятки человек погибли в давке...</td>\n",
       "      <td>РБК</td>\n",
       "      <td>None</td>\n",
       "      <td>http://top.rbc.ru/incidents/01/01/2013/839240....</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text agency author  \\\n",
       "0   1  Избежать \"фискального обрыва\": Сенат США подде...    РБК   None   \n",
       "1   2    \"Фискальный обрыв\" в США временно предотвращен.    РБК   None   \n",
       "2   3  Чечня попросила националистов составить кодекс...    РБК   None   \n",
       "3   4  Северокорейский лидер впервые за 19 лет поздра...    РБК   None   \n",
       "4   5  В Кот-Д`Ивуаре десятки человек погибли в давке...    РБК   None   \n",
       "\n",
       "                                                 url        date  \n",
       "0  http://top.rbc.ru/economics/01/01/2013/839229....  2013-01-01  \n",
       "1  http://top.rbc.ru/economics/01/01/2013/839223....  2013-01-01  \n",
       "2  http://top.rbc.ru/society/01/01/2013/839242.shtml  2013-01-01  \n",
       "3  http://top.rbc.ru/society/01/01/2013/839227.shtml  2013-01-01  \n",
       "4  http://top.rbc.ru/incidents/01/01/2013/839240....  2013-01-01  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"Sentences number: \", Sentences.shape\n",
    "Sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pair_id  id_1  id_2                                             text_1  \\\n",
      "0        1   201  8159  Полицейским разрешат стрелять на поражение по ...   \n",
      "1        2   202  8158  Право полицейских на проникновение в жилище ре...   \n",
      "2        3   273  8167  Президент Египта ввел чрезвычайное положение в...   \n",
      "3        4   220  8160  Вернувшихся из Сирии россиян волнует вопрос тр...   \n",
      "4        5   223  8160  В Москву из Сирии вернулись 2 самолета МЧС с р...   \n",
      "\n",
      "                                              text_2   jaccard  class  \n",
      "0  Полиции могут разрешить стрелять по хулиганам ...  0.650000      1  \n",
      "1  Правила внесудебного проникновения полицейских...  0.500000      1  \n",
      "2  Власти Египта угрожают ввести в стране чрезвыч...  0.611429      1  \n",
      "3  Самолеты МЧС вывезут россиян из разрушенной Си...  0.324037      0  \n",
      "4  Самолеты МЧС вывезут россиян из разрушенной Си...  0.606218      1  \n",
      "   id                                               text agency author  \\\n",
      "0   1  Избежать \"фискального обрыва\": Сенат США подде...    РБК    NaN   \n",
      "1   2    \"Фискальный обрыв\" в США временно предотвращен.    РБК    NaN   \n",
      "2   3  Чечня попросила националистов составить кодекс...    РБК    NaN   \n",
      "3   4  Северокорейский лидер впервые за 19 лет поздра...    РБК    NaN   \n",
      "4   5  В Кот-Д`Ивуаре десятки человек погибли в давке...    РБК    NaN   \n",
      "\n",
      "                                                 url        date  \n",
      "0  http://top.rbc.ru/economics/01/01/2013/839229....  2013-01-01  \n",
      "1  http://top.rbc.ru/economics/01/01/2013/839223....  2013-01-01  \n",
      "2  http://top.rbc.ru/society/01/01/2013/839242.shtml  2013-01-01  \n",
      "3  http://top.rbc.ru/society/01/01/2013/839227.shtml  2013-01-01  \n",
      "4  http://top.rbc.ru/incidents/01/01/2013/839240....  2013-01-01  \n"
     ]
    }
   ],
   "source": [
    "Paraphrases = pd.read_csv(\"Paraphrases.csv\")\n",
    "print Paraphrases.head()\n",
    "Sentences = pd.read_csv(\"Sentences.csv\")\n",
    "print Sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Lemmatization and constructing the inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "RusLem = pymorphy2.MorphAnalyzer()\n",
    "#обратный индес: {токен:список id предложений,в которых токен встречается}\n",
    "inverted_index = {}\n",
    "#прямой индекс: {id предложения:список токенов}\n",
    "forward_index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np_sentences = np.asarray(Sentences[['id', 'text']])\n",
    "\n",
    "for sent in np_sentences:\n",
    "    tokens = re.findall('[\\w]+',sent[1].decode(\"utf-8\").strip().lower(), re.U)\n",
    "    tokens = [RusLem.parse(token)[0].normal_form for token in tokens]\n",
    "    forward_index[sent[0]] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_inverted_index(forward_index):\n",
    "    inverted_index = {}\n",
    "    for sent_id in forward_index.keys():\n",
    "        for token in forward_index[sent_id]:\n",
    "            if token not in inverted_index:\n",
    "                inverted_index[token] = [sent_id]\n",
    "            elif sent_id not in inverted_index[token]:\n",
    "                inverted_index[token].append(sent_id)\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# bprint(np_sentences[:5, 1])\n",
    "inverted_index = create_inverted_index(forward_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(\"./forward_index\", 'wb') as res_file:\n",
    "    pickle.dump(forward_index, res_file)\n",
    "with open(\"./inverted_index\", 'wb') as res_file:\n",
    "    pickle.dump(inverted_index, res_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## String - based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25495 34424 34611]\n",
      " [25496 34596 34611]\n",
      " [25498 34488 34613]\n",
      " [25499 34614 34615]\n",
      " [25500 34616 34617]\n",
      " [25514 34622 34633]\n",
      " [25524 34566 34654]\n",
      " [25548 34519 34681]\n",
      " [25549 34565 34681]\n",
      " [25577 34584 34722]]\n"
     ]
    }
   ],
   "source": [
    "np_paraphrases = np.asarray(Paraphrases[['pair_id', 'id_1', 'id_2']])\n",
    "print np_paraphrases[len(np_paraphrases) - 10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![Image](img1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#возвращает значение признака для 1 пары предложений\n",
    "#paraphrase = [id_1, id_2]\n",
    "def get_string_feature(forward_index, paraphrase, n_gram = 1, gram_type = 'word', feat_num = 1):\n",
    "    id_1 = paraphrase[0]\n",
    "    id_2 = paraphrase[1]\n",
    "    if gram_type == 'word':\n",
    "        tokens_1 = forward_index[id_1]\n",
    "        tokens_2 = forward_index[id_2]\n",
    "        if n_gram == 1:\n",
    "            set_1 = set(tokens_1)\n",
    "            set_2 = set(tokens_2)\n",
    "        elif n_gram == 2:\n",
    "            set_1 = set([\" \".join([tokens_1[idx], tokens_1[idx + 1]]) for idx in range(len(tokens_1) - 1)])\n",
    "            set_2 = set([\" \".join([tokens_2[idx], tokens_2[idx + 1]]) for idx in range(len(tokens_2) - 1)])\n",
    "        elif n_gram == 3:\n",
    "            set_1 = set([\" \".join([tokens_1[idx], tokens_1[idx + 1], tokens_1[idx + 2]]) \\\n",
    "                                                     for idx in range(len(tokens_1) - 2)])\n",
    "            set_2 = set([\" \".join([tokens_2[idx], tokens_2[idx + 1], tokens_2[idx + 2]])\\\n",
    "                                                     for idx in range(len(tokens_2) - 2)])\n",
    "        else:\n",
    "            print \"Not correct n_gram parameter\"\n",
    "            return None\n",
    "\n",
    "    elif gram_type == 'symbol':\n",
    "        text_1 = \" \".join(forward_index[id_1])\n",
    "        text_2 = \" \".join(forward_index[id_2])\n",
    "        if n_gram == 2:\n",
    "            set_1 = set([\"\".join([text_1[idx], text_1[idx + 1]]) for idx in range(len(text_1) - 1)])\n",
    "            set_2 = set([\"\".join([text_2[idx], text_2[idx + 1]]) for idx in range(len(text_2) - 1)])\n",
    "        elif n_gram == 3:\n",
    "            set_1 = set([\"\".join([text_1[idx], text_1[idx + 1], text_1[idx + 2]])\\\n",
    "                         for idx in range(len(text_1) - 2)])\n",
    "            set_2 = set([\"\".join([text_2[idx], text_2[idx + 1], text_2[idx + 2]])\\\n",
    "                         for idx in range(len(text_2) - 2)])\n",
    "        else:\n",
    "            print \"Not correct n_gram parameter\"\n",
    "            return None\n",
    "    else:\n",
    "        print \"Not correct gram_type parameter\"\n",
    "        return None    \n",
    "\n",
    "    if feat_num == 1:\n",
    "        feature = len(set_1.intersection(set_2)) / float(len(set_1.union(set_2))) \\\n",
    "                                                    if len(set_1.union(set_2)) != 0 else 0\n",
    "    elif feat_num == 2:\n",
    "        feature = len(set_1.intersection(set_2)) / float(len(set_1)) if len(set_1) != 0 else 0\n",
    "    elif feat_num == 3:\n",
    "        feature = len(set_1.intersection(set_2)) / float(len(set_2)) if len(set_2) != 0 else 0\n",
    "    else:\n",
    "        print \"Not correct feat_num parameter\"\n",
    "        return None\n",
    "    \n",
    "    return feature\n",
    "\n",
    "#подсчет string - based features для всего датасета перифраз *создание numpy array*\n",
    "#всего 15 фичей: 9 для словарных N-Gram(1, 2, 3 слов), 6 для символьных(2 и 3 символов)\n",
    "def get_string_feature_for_all(forward_index, paraphrases):\n",
    "    all_string_features = []\n",
    "    for paraphrase in paraphrases:\n",
    "#         features = [paraphrase[0]]\n",
    "#         print \"PAIR_ID\", paraphrase[0]\n",
    "#         bprint(forward_index[paraphrase[1]])\n",
    "#         bprint(forward_index[paraphrase[2]])\n",
    "        features = []\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=1, gram_type='word', feat_num=1))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=2, gram_type='word', feat_num=1))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=3, gram_type='word', feat_num=1))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=1, gram_type='word', feat_num=2))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=2, gram_type='word', feat_num=2))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=3, gram_type='word', feat_num=2))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=1, gram_type='word', feat_num=3))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=2, gram_type='word', feat_num=3))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=3, gram_type='word', feat_num=3))\n",
    "        \n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=2, gram_type='symbol', feat_num=1))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=3, gram_type='symbol', feat_num=1))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=2, gram_type='symbol', feat_num=2))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=3, gram_type='symbol', feat_num=2))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=2, gram_type='symbol', feat_num=3))\n",
    "        features.append(get_string_feature(forward_index, paraphrase[1:], n_gram=3, gram_type='symbol', feat_num=3))\n",
    "        all_string_features.append(features)\n",
    "    return np.asarray(all_string_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "полицейский\n",
      "разрешить\n",
      "стрелять\n",
      "на\n",
      "поражение\n",
      "по\n",
      "гражданин\n",
      "с\n",
      "травматика\n",
      "\n",
      "полиция\n",
      "мочь\n",
      "разрешить\n",
      "стрелять\n",
      "по\n",
      "хулиган\n",
      "с\n",
      "травматика\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4146341463414634"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bprint(forward_index[np_paraphrases[0, 1]])\n",
    "print \n",
    "bprint(forward_index[np_paraphrases[0, 2]])\n",
    "get_string_feature(forward_index, [np_paraphrases[0, 1], np_paraphrases[0, 2]], n_gram=3, gram_type='symbol', feat_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "string_features = get_string_feature_for_all(forward_index, np_paraphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7227, 15)\n"
     ]
    }
   ],
   "source": [
    "print np.asarray(string_features).shape\n",
    "with open(\"./string_features\", 'wb') as res_file:\n",
    "    pickle.dump(string_features, res_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## IR features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "    N - количество предложений \n",
    "    N(w_i) - количество предложений в которых встречается терм\n",
    "    avg - средняя длина предложения в датасете"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![Image](./img2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#количество предложений\n",
    "N = len(forward_index.keys())\n",
    "\n",
    "avg = 0\n",
    "for sent in forward_index.keys():\n",
    "    avg += len(forward_index[sent])\n",
    "    \n",
    "#средняя длина датасета\n",
    "avg /= float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_TF(token, sent_tokens):\n",
    "    tf = 0\n",
    "    for t in sent_tokens:\n",
    "        if t == token:\n",
    "            tf += 1\n",
    "    return tf\n",
    "\n",
    "def create_IDF(inverted_index):\n",
    "    # IDF = {токен: его IDF}\n",
    "    IDF = {}\n",
    "    for token in inverted_index.keys():\n",
    "        IDF[token] = np.log((N - len(inverted_index[token]) + 0.5) / float(len(inverted_index[token]) + 0.5))\n",
    "    return IDF\n",
    "        \n",
    "def BM25(id_1, id_2, forward_index, IDF, k = 1.2, b = 0.75):\n",
    "    tokens_1 = forward_index[id_1]\n",
    "    tokens_2 = forward_index[id_2]\n",
    "#     bprint(tokens_1)\n",
    "#     print\n",
    "#     bprint(tokens_2)\n",
    "    bm25 = 0\n",
    "    for token in tokens_1:\n",
    "        tf = get_TF(token, tokens_2)\n",
    "        tmp = (tf * (k + 1)) / float(tf + k * (1 - b + b * len(tokens_2)/avg))\n",
    "        bm25 += IDF[token] * tmp\n",
    "    return bm25\n",
    "\n",
    "#максимальный IDF слов, которыми различаются 2 предложения\n",
    "def maxIDF(id_1, id_2, forward_index, IDF):\n",
    "    tokens_1 = set(forward_index[id_1])\n",
    "    tokens_2 = set(forward_index[id_2])\n",
    "    difference = list(tokens_1.symmetric_difference(tokens_2))\n",
    "    if len(difference) == 0:\n",
    "        return 0\n",
    "    difference_IDF = [IDF[token] for token in difference]\n",
    "    return max(difference_IDF)\n",
    "\n",
    "#сумма IDF слов, которыми различаются 2 предложения\n",
    "def sumIDF(id_1, id_2, forward_index, IDF):\n",
    "    tokens_1 = set(forward_index[id_1])\n",
    "    tokens_2 = set(forward_index[id_2])\n",
    "    difference = list(tokens_1.symmetric_difference(tokens_2))\n",
    "    difference_IDF = [IDF[token] for token in difference]\n",
    "    return sum(difference_IDF)\n",
    "\n",
    "#формирует список всех IR фичей - BM25, maxIDF, sumIDF\n",
    "def get_IR_features_all(forward_index, inverted_index, paraphrases):\n",
    "    all_string_features = []\n",
    "    IDF = create_IDF(inverted_index)\n",
    "#     print IDF\n",
    "    for paraphrase in paraphrases:\n",
    "        id_1 = paraphrase[1]\n",
    "        id_2 = paraphrase[2]\n",
    "        features = []\n",
    "        features.append(BM25(id_1, id_2, forward_index, IDF))\n",
    "        features.append(maxIDF(id_1, id_2, forward_index, IDF))\n",
    "        features.append(sumIDF(id_1, id_2, forward_index, IDF))\n",
    "        all_string_features.append(features)\n",
    "    return np.asarray(all_string_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "IR_features = get_IR_features_all(forward_index, inverted_index, np_paraphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 25.51432783   7.38237325  34.89266782]\n",
      " [ 22.21936364   8.99230873  42.4968678 ]\n",
      " [ 23.43991854   8.4814002   34.48990074]\n",
      " [ 12.27017085   8.99230873  60.84430134]\n",
      " [ 21.92113702   6.87113279  36.32048699]]\n"
     ]
    }
   ],
   "source": [
    "print IR_features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.96857900846\n"
     ]
    }
   ],
   "source": [
    "print avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#выгрузка данных\n",
    "with open(\"./forward_index\", 'r') as res_file:\n",
    "    forward_index = pickle.load(res_file)\n",
    "with open(\"./inverted_index\", 'r') as res_file:\n",
    "    inverted_index = pickle.load(res_file)\n",
    "with open(\"./string_features\", 'r') as res_file:\n",
    "    string_features = pickle.load(res_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_test_data(class_cnt = 2):\n",
    "    tree = etree.parse('paraphrases_gold.xml')  \n",
    "    root = tree.getroot() \n",
    "    corpus = root[0]\n",
    "    data = []\n",
    "\n",
    "    for paraphrase in corpus:\n",
    "        new_pair_data = []\n",
    "        for field in paraphrase:\n",
    "            new_pair_data.append(field.text.encode('utf-8'))\n",
    "        data.append(new_pair_data)\n",
    "\n",
    "    Paraphrases = pd.DataFrame(np.asarray(data), columns = ['pair_id', 'id_1', 'id_2', 'text_1', 'text_2', 'class'])\n",
    "    Paraphrases[['pair_id', 'id_1', 'id_2', 'class']] = \\\n",
    "            Paraphrases[['pair_id', 'id_1', 'id_2', 'class']].apply(pd.to_numeric)\n",
    "    if class_cnt == 2:\n",
    "        Paraphrases['class'] = Paraphrases['class'].apply(lambda x: 1 if x >=0 else 0)\n",
    "    Paraphrases.to_csv(\"Paraphrases_test.csv\", index=False, encoding='utf-8')\n",
    "    return Paraphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np_test_data = np.asarray(get_test_data())\n",
    "# np_test_sentences = np_test_data[:,[1, 2]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#обратный индекс для тестовых данных: {токен:список id предложений,в которых токен встречается}\n",
    "test_inverted_index = {}\n",
    "#прямой индекс для тестовых данных: {id предложения:список токенов}\n",
    "test_forward_index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for pair in np_test_data:\n",
    "    tokens = re.findall('[\\w]+',pair[3].decode(\"utf-8\").strip().lower(), re.U)\n",
    "    tokens = [RusLem.parse(token)[0].normal_form for token in tokens]\n",
    "    test_forward_index[pair[1]] = tokens\n",
    "    tokens = re.findall('[\\w]+',pair[4].decode(\"utf-8\").strip().lower(), re.U)\n",
    "    tokens = [RusLem.parse(token)[0].normal_form for token in tokens]\n",
    "    test_forward_index[pair[2]] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_inverted_index = create_inverted_index(test_forward_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in WV: 45\n",
      "All words: 4239\n"
     ]
    }
   ],
   "source": [
    "test_embedding_index = {}\n",
    "#создание прямого индекса из embedding слов\n",
    "cnt = 0\n",
    "for idx, word in enumerate(test_inverted_index.keys()):\n",
    "    if word in WordVectors:\n",
    "        test_embedding_index[word] = WordVectors[word]\n",
    "    else:\n",
    "        cnt += 1\n",
    "print \"Not in WV:\", cnt\n",
    "print \"All words:\", len(test_inverted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"test_embedding_index\", \"w\") as f:\n",
    "    pickle.dump(test_embedding_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_string_features = get_string_feature_for_all(test_forward_index, np_test_data[:, :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_IR_features = get_IR_features_all(test_forward_index, test_inverted_index, np_test_data[:, :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_mean_cosine_W2V = get_mean_embedding(np_test_data[:, :3], test_forward_index, test_embedding_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_max_sim_W2V = get_max_similarity(np_test_data[:, :3], test_forward_index, test_embedding_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_W2V_words_sim = get_words_similarity(np_test_data[:, :3], test_forward_index, test_embedding_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_all_features = np.concatenate((test_string_features, test_IR_features, \\\n",
    "                                    test_max_sim_W2V, test_W2V_words_sim), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_mean_cosine_W2V_expand = get_mean_embedding(np_test_data[:, :3], \\\n",
    "                                                 test_forward_index, test_embedding_index, need_vec=True)\n",
    "test_all_features_expand = np.concatenate((test_string_features, test_IR_features, test_mean_cosine_W2V_expand), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1924, 125)\n",
      "(1924, 15)\n",
      "(1924, 3)\n",
      "(1924, 1)\n",
      "(1924, 241)\n",
      "(1924, 2)\n"
     ]
    }
   ],
   "source": [
    "print test_all_features.shape\n",
    "print test_string_features.shape\n",
    "print test_IR_features.shape\n",
    "print test_mean_cosine_W2V.shape\n",
    "print test_mean_cosine_W2V_expand.shape\n",
    "print test_max_sim_W2V.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Тrain Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Only String Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "target = np.asarray(Paraphrases[['class']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7227, 15)\n",
      "(7227, 1)\n"
     ]
    }
   ],
   "source": [
    "print string_features.shape\n",
    "print target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "string_model = GradientBoostingClassifier(loss = 'deviance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "parametrs_grid = {'learning_rate':[0.001,0.005, 0.01, 0.05, 0.1], 'n_estimators': [100, 300, 600, 800, 1000, 1200, 1400],\\\n",
    "                 'subsample':[0.6, 0.7, 0.8, 1.], 'max_depth':[2, 3, 4, 5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 560 candidates, totalling 3920 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:   19.1s\n",
      "[Parallel(n_jobs=10)]: Done 180 tasks      | elapsed:  8.4min\n",
      "[Parallel(n_jobs=10)]: Done 430 tasks      | elapsed: 23.8min\n",
      "[Parallel(n_jobs=10)]: Done 780 tasks      | elapsed: 68.3min\n",
      "[Parallel(n_jobs=10)]: Done 1230 tasks      | elapsed: 93.2min\n",
      "[Parallel(n_jobs=10)]: Done 1780 tasks      | elapsed: 144.6min\n",
      "[Parallel(n_jobs=10)]: Done 2430 tasks      | elapsed: 201.9min\n",
      "[Parallel(n_jobs=10)]: Done 3180 tasks      | elapsed: 266.1min\n",
      "[Parallel(n_jobs=10)]: Done 3920 out of 3920 | elapsed: 329.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=7, error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=100, presort='auto', random_state=None,\n",
       "              subsample=1.0, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=10,\n",
       "       param_grid={'n_estimators': [100, 300, 600, 800, 1000, 1200, 1400], 'subsample': [0.6, 0.7, 0.8, 1.0], 'learning_rate': [0.001, 0.005, 0.01, 0.05, 0.1], 'max_depth': [2, 3, 4, 5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=make_scorer(f1_score), verbose=1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_metric = make_scorer(f1_score)\n",
    "grid_model = GridSearchCV(string_model, parametrs_grid, cv = 7, verbose = 1, scoring=f1_metric, n_jobs=10)\n",
    "grid_model.fit(string_features, target.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 100, 'subsample': 0.6}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.01, loss='deviance', max_depth=4,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=100, presort='auto', random_state=None,\n",
       "              subsample=0.6, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_GB = GradientBoostingClassifier(loss = 'deviance', learning_rate=0.01, \\\n",
    "                                     max_depth=4, n_estimators=100, subsample=0.6)\n",
    "best_GB.fit(string_features, target.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST Gradient Boosting Result on String Features\n",
      "F1-score 0.789415656009\n",
      "Accuracy: 0.702182952183\n"
     ]
    }
   ],
   "source": [
    "test_predict = best_GB.predict(test_string_features)\n",
    "print \"BEST Gradient Boosting Result on String Features\"\n",
    "print \"F1-score\", f1_score(np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict)\n",
    "print \"Accuracy:\", accuracy_score(np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict)\n",
    "# print (classification_report( np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict, target_names= ['non-paraphrases', 'paraphrases']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78941565600882035"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with openen(\"string_model\", \"wb\") as res_file:\n",
    "    pickle.dump(best_GBres_files_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7227, 3)\n",
      "(7227, 1)\n"
     ]
    }
   ],
   "source": [
    "target = np.asarray(Paraphrases[['class']])\n",
    "print IR_features.shape\n",
    "print target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Only IR features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "IR_model = GradientBoostingClassifier(loss = 'deviance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "parametrs_grid = {'learning_rate':[0.005, 0.01, 0.1], 'n_estimators': [100, 300, 500, 700, 1000, 1300],\\\n",
    "                 'subsample':[0.6, 0.8, 1.], 'max_depth':[2, 3, 4, 5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 216 candidates, totalling 1512 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:   16.6s\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed: 12.1min\n",
      "[Parallel(n_jobs=5)]: Done 790 tasks      | elapsed: 23.1min\n",
      "[Parallel(n_jobs=5)]: Done 1240 tasks      | elapsed: 38.0min\n",
      "[Parallel(n_jobs=5)]: Done 1512 out of 1512 | elapsed: 48.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=7, error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=100, presort='auto', random_state=None,\n",
       "              subsample=1.0, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=5,\n",
       "       param_grid={'n_estimators': [100, 300, 500, 700, 1000, 1300], 'subsample': [0.6, 0.8, 1.0], 'learning_rate': [0.005, 0.01, 0.1], 'max_depth': [2, 3, 4, 5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=make_scorer(f1_score), verbose=1)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_metric = make_scorer(f1_score)\n",
    "grid_model = GridSearchCV(IR_model, parametrs_grid, cv = 7, verbose = 1, scoring=f1_metric, n_jobs=5)\n",
    "grid_model.fit(IR_features, target.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST Gradient Boosting Result on IR Features\n",
      "\n",
      "F1-score 0.779085872576\n",
      "Accuracy: 0.668399168399\n"
     ]
    }
   ],
   "source": [
    "best_GB_IR = GradientBoostingClassifier(loss = 'deviance', learning_rate=0.01, \\\n",
    "                                     max_depth=3, n_estimators=100, subsample=1.0)\n",
    "best_GB_IR.fit(IR_features, target.ravel())\n",
    "\n",
    "test_predict = best_GB_IR.predict(test_IR_features)\n",
    "print \"BEST Gradient Boosting Result on IR Features\"\n",
    "print \n",
    "print \"F1-score\", f1_score(np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict)\n",
    "print \"Accuracy:\", accuracy_score(np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict)\n",
    "# print (classification_report( np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict, target_names= ['non-paraphrases', 'paraphrases']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### String + IR features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7227, 18)\n"
     ]
    }
   ],
   "source": [
    "all_features = np.concatenate((string_features, IR_features), axis = 1)\n",
    "print all_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "string_IR_model = GradientBoostingClassifier(loss = 'deviance')\n",
    "parametrs_grid = {'learning_rate':[0.005, 0.01, 0.05], 'n_estimators': [100, 300, 500, 700, 1000, 1300],\\\n",
    "                 'subsample':[0.6, 0.7, 0.8, 1.], 'max_depth':[2, 3, 4, 5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 288 candidates, totalling 2016 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:   33.1s\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed: 27.0min\n",
      "[Parallel(n_jobs=5)]: Done 790 tasks      | elapsed: 64.1min\n",
      "[Parallel(n_jobs=5)]: Done 1240 tasks      | elapsed: 97.8min\n",
      "[Parallel(n_jobs=5)]: Done 1790 tasks      | elapsed: 140.9min\n",
      "[Parallel(n_jobs=5)]: Done 2016 out of 2016 | elapsed: 170.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=7, error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=100, presort='auto', random_state=None,\n",
       "              subsample=1.0, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=5,\n",
       "       param_grid={'n_estimators': [100, 300, 500, 700, 1000, 1300], 'subsample': [0.6, 0.7, 0.8, 1.0], 'learning_rate': [0.005, 0.01, 0.05], 'max_depth': [2, 3, 4, 5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=make_scorer(f1_score), verbose=1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_metric = make_scorer(f1_score)\n",
    "grid_model = GridSearchCV(string_IR_model, parametrs_grid, cv = 7, verbose = 1, n_jobs=5, scoring=f1_metric)\n",
    "grid_model.fit(all_features, target.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.005, 'max_depth': 2, 'n_estimators': 500, 'subsample': 0.7}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST Gradient Boosting Result on String+IR Features\n",
      "\n",
      "F1-score 0.80221402214\n",
      "Accuracy: 0.721413721414\n"
     ]
    }
   ],
   "source": [
    "best_GB_IR_string = GradientBoostingClassifier(loss = 'deviance', learning_rate=0.05, \\\n",
    "                                     max_depth=2, n_estimators=500, subsample=0.7)\n",
    "best_GB_IR_string.fit(all_features, target.ravel())\n",
    "\n",
    "test_predict = best_GB_IR_string.predict(test_all_features)\n",
    "print \"BEST Gradient Boosting Result on String+IR Features\"\n",
    "print \n",
    "print \"F1-score\", f1_score(np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict)\n",
    "print \"Accuracy:\", accuracy_score(np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Experiments with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291584\r\n"
     ]
    }
   ],
   "source": [
    "! ls -1 ../news | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RusLem = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Избежать \"фискального обрыва\": Сенат США поддержал повышение налогов.\n",
      "None\n",
      "12062\n",
      "избежать\n",
      "фискальный\n",
      "обрыв\n",
      "сенат\n",
      "сша\n",
      "поддержать\n",
      "повышение\n",
      "налог\n",
      "None\n",
      "12062\n"
     ]
    }
   ],
   "source": [
    "#формирование датасета для W2V из тренировочной выборки\n",
    "sentences_text = np.asarray(Sentences[['text']])\n",
    "print bprint(sentences_text[0])\n",
    "print len(sentences_text)\n",
    "sentences = []\n",
    "\n",
    "for sent in sentences_text:\n",
    "    tokens = re.findall('[\\w]+',sent[0].decode(\"utf-8\").strip().lower(), re.U)\n",
    "    tokens = [RusLem.parse(token)[0].normal_form for token in tokens]\n",
    "    sentences.append(tokens)\n",
    "    \n",
    "print bprint(sentences[0])\n",
    "print len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare 0 file\n",
      "0 news_text\n",
      "Prepare 30000 file\n",
      "164446 news_text\n",
      "Prepare 60000 file\n",
      "330209 news_text\n",
      "Prepare 90000 file\n",
      "496602 news_text\n",
      "Prepare 120000 file\n",
      "662281 news_text\n",
      "Prepare 150000 file\n",
      "827605 news_text\n",
      "Prepare 180000 file\n",
      "988928 news_text\n",
      "Prepare 210000 file\n",
      "1154258 news_text\n",
      "Prepare 240000 file\n",
      "1319022 news_text\n",
      "Prepare 270000 file\n",
      "1483091 news_text\n",
      "В столице Ирака Багдаде в воскресенье на одном из рынков произошел взрыв, в результате которого 8 человек погибли. Об этом сообщает РИА Новости . читайте также \n",
      "Согласно имеющейся информации, взрывное устройство было спрятано в автомобиле. Сведений о числе раненных пока нет. \n",
      "Полиция ведет расследование. На место также прибыли машины скорой помощи. \n",
      "Ни одна из террористических группировок пока не взяла на себя ответственность за произошедшее.\n",
      "\n",
      "На рынке в Багдаде прогремел взрыв: 8 человек погибли\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'social',\n",
       " u'organizations',\n",
       " u'language',\n",
       " u'uuid',\n",
       " u'thread',\n",
       " u'author',\n",
       " u'url',\n",
       " u'ord_in_thread',\n",
       " u'title',\n",
       " u'locations',\n",
       " u'highlightText',\n",
       " u'entities',\n",
       " u'persons',\n",
       " u'external_links',\n",
       " u'text',\n",
       " u'crawled',\n",
       " u'highlightTitle',\n",
       " u'published']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#предобработка датасета новостных статей\n",
    "news_files = os.listdir(\"../news\")\n",
    "# news_text = \"\"\n",
    "with io.open(\"news_text\", 'w', encoding='utf-8') as news_text_file:\n",
    "    for idx, f in enumerate(news_files):\n",
    "        if idx % 30000 == 0:\n",
    "            print \"Prepare\", idx, 'file'\n",
    "            !wc -l news_text\n",
    "        with open(\"../news/\" + f, 'r') as news_file:\n",
    "            js = json.load(news_file, encoding='utf-8')\n",
    "            news_text_file.write(js['text'] +  \\\n",
    "                                 js['title'] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1603788 news_text\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l news_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prepare 543300 line\tAll Sentences:1999447"
     ]
    }
   ],
   "source": [
    "#формирование данных для обучения Word2Vec из датасета новостных статей на русском\n",
    "# new_sentences = []\n",
    "with open(\"./news_text\", \"r\") as news_text:\n",
    "    for ind, line in enumerate(news_text):\n",
    "        if ind < 300400:\n",
    "            continue\n",
    "        if ind % 100 == 0:\n",
    "#             logging.info(\"Prepare %d\" % ind)\n",
    "            sys.stderr.write(\"\\rPrepare %d line\" % ind + \"\\tAll Sentences:\" + str(len(new_sentences)))\n",
    "        for sent in line.strip().split('.'):\n",
    "#             print sent\n",
    "            tokens = re.findall('[\\w]+',sent.decode(\"utf-8\").strip().lower(), re.U)\n",
    "            tokens = [RusLem.parse(token)[0].normal_form for token in tokens]\n",
    "            new_sentences.append(tokens)\n",
    "        if len(new_sentences) > 2000000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "фото\n",
      "андрей\n",
      "сенький\n",
      "2000003\n"
     ]
    }
   ],
   "source": [
    "bprint(new_sentences[1200])\n",
    "print len(new_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# del(all_sentences)\n",
    "del(new_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500005\n"
     ]
    }
   ],
   "source": [
    "print len(new_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#объединение тренировочного датасета и датасета новостей\n",
    "all_sentences = new_sentences + sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12062\n"
     ]
    }
   ],
   "source": [
    "print len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del(model)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#обучение Word2Vec\n",
    "model = Word2Vec(size=120, window=5, min_count=0, iter = 5)\n",
    "model.build_vocab(sentences, update=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500002\n",
      "1\n",
      "500001\n",
      "2\n",
      "500025\n",
      "3\n",
      "500002\n",
      "4\n",
      "500001\n",
      "5\n",
      "500002\n",
      "6\n",
      "500001\n",
      "7\n",
      "500001\n",
      "8\n",
      "500001\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    print i\n",
    "    try:\n",
    "        del(new_sentences)\n",
    "    except:\n",
    "        pass\n",
    "    gc.collect()\n",
    "    with open('news_sent_' + str(i), \"r\") as save_file:\n",
    "        new_sentences = pickle.load(save_file)\n",
    "        print len(new_sentences)\n",
    "        model.build_vocab(new_sentences, update=True)\n",
    "        model.train(new_sentences, total_examples=len(new_sentences), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388390\n",
      "10136\n"
     ]
    }
   ],
   "source": [
    "WordVectors = model.wv\n",
    "vocab = WordVectors.vocab\n",
    "print len(model.wv.vocab.keys())\n",
    "print len(inverted_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "государство 0.709696948528\n",
      "регион 0.663186311722\n",
      "континент 0.594460070133\n",
      "украина 0.562653005123\n",
      "азербайджан 0.551063299179\n",
      "россия 0.550385415554\n",
      "латвия 0.533351719379\n",
      "евросоюз 0.527279853821\n",
      "турция 0.513354480267\n",
      "америка 0.511987745762\n",
      "\n",
      "американский 0.726210474968\n",
      "вашингтон 0.697807848454\n",
      "американец 0.636370003223\n",
      "турция 0.618628025055\n",
      "пентагон 0.600299835205\n",
      "иран 0.597927153111\n",
      "америка 0.592969298363\n",
      "китай 0.591142654419\n",
      "сирия 0.590399324894\n",
      "филиппина 0.585408568382\n"
     ]
    }
   ],
   "source": [
    "x = WordVectors.most_similar(u'страна')\n",
    "for w in x:\n",
    "    print w[0], w[1]\n",
    "\n",
    "print\n",
    "x = WordVectors.most_similar(u'сша')\n",
    "for w in x:\n",
    "    print w[0], w[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5667"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del(new_sentences)\n",
    "del(RusLem)\n",
    "del(sentences_text)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('word_vectors', \"w\") as f:\n",
    "    pickle.dump(WordVectors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for idx, word in enumerate(inverted_index.keys()):\n",
    "#     if idx % 100 == 0:\n",
    "#         print idx\n",
    "    if word not in vocab:\n",
    "        cnt += 1\n",
    "print cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_index = {}\n",
    "#создание прямого индекса из embedding слов\n",
    "for idx, word in enumerate(inverted_index.keys()):\n",
    "    if word in WordVectors:\n",
    "        embedding_index[word] = WordVectors[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1.03948164e+00  -4.67318296e-01   1.66246280e-01  -2.52881908e+00\n",
      "   3.44445854e-01  -2.05386305e+00  -6.67268336e-02   3.26674294e+00\n",
      "  -2.14855886e+00  -5.98546386e-01  -6.41767263e-01   1.40415490e+00\n",
      "   1.69666260e-02   8.70592594e-01  -1.65405655e+00  -8.84563386e-01\n",
      "   1.84067285e+00   1.03923178e+00   1.37990391e+00   6.19873703e-01\n",
      "  -2.81489462e-01  -1.69963147e-02   1.75444233e+00  -5.43999001e-02\n",
      "   1.79508877e+00   6.65230393e-01  -7.38968730e-01   2.70580888e-01\n",
      "   6.00606441e-01  -1.12671936e+00  -4.34072447e+00   3.51008654e-01\n",
      "  -5.05638659e-01   1.39639014e-03   1.01712763e+00  -7.31826186e-01\n",
      "  -1.21730790e-01   8.53951395e-01   1.14186680e+00   1.07774682e-01\n",
      "  -2.17465115e+00  -1.13333440e+00   2.38550514e-01   1.92197824e+00\n",
      "  -2.14486527e+00   1.51425824e-01   3.99049163e-01   2.66202474e+00\n",
      "   4.43523079e-01  -1.30428648e+00   5.11890113e-01   2.12164545e+00\n",
      "  -8.19957197e-01  -4.89127904e-01  -3.33053827e-01  -1.97383359e-01\n",
      "  -1.26978791e+00  -9.82274413e-01  -1.32783544e+00  -1.23387694e-01\n",
      "   2.94958520e+00   6.22885823e-01   1.25763047e+00   1.90394187e+00\n",
      "  -4.36612338e-01   3.29820603e-01   2.16358519e+00   2.41726115e-01\n",
      "  -7.10147381e-01  -5.64137936e-01  -1.05338931e+00  -2.76688218e+00\n",
      "  -3.68085980e+00   1.02226877e+00  -2.67606735e-01   9.50792730e-01\n",
      "   9.58112121e-01  -1.33279896e+00   2.27848589e-01   1.22035706e+00\n",
      "  -2.26004338e+00  -1.02077961e+00  -2.35770941e-01   3.62287283e-01\n",
      "  -2.53575277e+00   1.64421529e-01  -1.15737152e+00  -1.82074368e-01\n",
      "  -2.25729585e+00  -4.19313192e-01   1.06900084e+00   2.56260800e+00\n",
      "  -3.46649742e+00  -2.58722246e-01  -8.58650863e-01  -7.72074699e-01\n",
      "   6.77995980e-01  -3.41997981e-01  -9.70689356e-02  -8.26567948e-01\n",
      "   1.85404539e+00   1.79131413e+00  -2.47599203e-02   4.40191954e-01\n",
      "  -9.47274268e-01   1.18852174e+00   2.82492965e-01   5.60907483e-01\n",
      "   9.34628725e-01  -7.23040521e-01  -1.64539754e+00   3.50831747e-01\n",
      "  -9.45825219e-01   8.03227305e-01  -5.61522186e-01  -1.57008413e-02\n",
      "   2.44367242e+00   3.12411845e-01  -1.29737365e+00  -1.96380651e+00]\n"
     ]
    }
   ],
   "source": [
    "print embedding_index[u'\\u0440\\u0430\\u0441\\u0442\\u0435\\u043d\\u0438\\u0435']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('word_embedding', \"w\") as f:\n",
    "    pickle.dump(embedding_index, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможные признаки:\n",
    "    1. Усреднение векторов всех слов\n",
    "    2. Максимальная близость между двумя словами в предложении - ?? \n",
    "    3. Максимальная близость между словами, которыми отличаются 2 предложения\n",
    "    4. Взвешенная сумма векторов слов: умноженное на IDF слова\n",
    "    3. Попарная близость все слов 2-х предложений -> N * N фичей, где N - максимальное количество слов в предложении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('word_embedding', \"r\") as f:\n",
    "    embedding_index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_mean_embedding(paraphrases, forward_index, embedding_index, need_vec = False):\n",
    "    res_feature = []\n",
    "    for pair in paraphrases:\n",
    "        embeddings1 = [embedding_index[word] if word in embedding_index else np.asarray([0] * 120)\\\n",
    "                       for word in forward_index[pair[1]]]\n",
    "        embeddings2 = [embedding_index[word] if word in embedding_index else np.asarray([0] * 120)\\\n",
    "                       for word in forward_index[pair[2]]]\n",
    "#         print np.asarray(embeddings1).shape\n",
    "        mean_1 = np.asarray(embeddings1).mean(axis = 0)\n",
    "        mean_2 = np.asarray(embeddings2).mean(axis = 0)\n",
    "        if not need_vec:\n",
    "            res_feature.append([1 - scipy.spatial.distance.cosine(mean_1, mean_2)])\n",
    "        else:\n",
    "            res_feature.append(list(mean_1) + list(mean_2) + [1 -scipy.spatial.distance.cosine(mean_1, mean_2)])\n",
    "    return np.asarray(res_feature)\n",
    "\n",
    "#2 фичи: максимальная близость слов в предложении + максимальная близость слов в разнице между предложениями\n",
    "def get_max_similarity(paraphrases, forward_index, embedding_index):\n",
    "    res_feature = []\n",
    "    for pair in paraphrases:\n",
    "        embeddings1 = [embedding_index[word] if word in embedding_index else np.asarray([0] * 120)\\\n",
    "                       for word in forward_index[pair[1]]]\n",
    "        embeddings2 = [embedding_index[word] if word in embedding_index else np.asarray([0] * 120)\\\n",
    "                       for word in forward_index[pair[2]]]\n",
    "        \n",
    "        diff = set(forward_index[pair[1]]).symmetric_difference(set(forward_index[pair[2]]))\n",
    "        idxs_1 = [i for i, w in enumerate(forward_index[pair[1]]) if w in diff]\n",
    "        idxs_2 = [i for i, w in enumerate(forward_index[pair[2]]) if w in diff]\n",
    "\n",
    "        max_similarity = -1\n",
    "        max_similarity_diff = -1\n",
    "        for idx_1, w1 in enumerate(embeddings1):\n",
    "            for idx_2, w2 in enumerate(embeddings2):\n",
    "                sim = 1 - scipy.spatial.distance.cosine(w1, w2)\n",
    "                if sim > max_similarity:\n",
    "                    max_similarity = sim\n",
    "                if idx_1 in idxs_1 and idx_2 in idxs_2:\n",
    "                    if sim > max_similarity_diff:\n",
    "                        max_similarity_diff = sim\n",
    "        res_feature.append([max_similarity, max_similarity_diff])\n",
    "        \n",
    "    return np.asarray(res_feature)\n",
    "\n",
    "def get_words_similarity(paraphrases, forward_index, embedding_index, max_words = 15):\n",
    "    res_feature = []\n",
    "    for pair in paraphrases:\n",
    "        embeddings1 = [embedding_index[word] if word in embedding_index else np.asarray([0.000001] * 120)\\\n",
    "                       for word in forward_index[pair[1]]]\n",
    "        embeddings2 = [embedding_index[word] if word in embedding_index else np.asarray([0.000001] * 120)\\\n",
    "                       for word in forward_index[pair[2]]]\n",
    "        \n",
    "#         bprint(forward_index[pair[1]])\n",
    "#         print \n",
    "#         bprint(forward_index[pair[2]])\n",
    "        feature = []\n",
    "        for idx_1 in range(max_words):\n",
    "            for idx_2 in range(idx_1 + 1, max_words):\n",
    "                if len(embeddings1) <= idx_1 or len(embeddings2) <= idx_2:\n",
    "                    feature.append(0)\n",
    "                    continue\n",
    "                    \n",
    "#                 print scipy.spatial.distance.cosine(embeddings1[idx_1], embeddings2[idx_2])\n",
    "#                 print embeddings1[idx_1]\n",
    "#                 print embeddings2[idx_2]\n",
    "                feature.append(1 - scipy.spatial.distance.cosine(embeddings1[idx_1], embeddings2[idx_2]))\n",
    "                \n",
    "        res_feature.append(feature)\n",
    "    return np.asarray(res_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Mean_W2V_feature = get_mean_embedding(np_paraphrases, forward_index, embedding_index, need_vec=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Mean_W2V_feature_expand = get_mean_embedding(np_paraphrases, forward_index, embedding_index, need_vec=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W2V_max_sim_feature = get_max_similarity(np_paraphrases, forward_index, embedding_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W2V_words_sim = get_words_similarity(np_paraphrases, forward_index, embedding_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1.02471026e-01   1.75180030e-01   3.94775927e-02   1.24003998e-01\n",
      "   2.19638087e-01   1.40358980e-01   5.50385420e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   1.06130852e-01   2.97192820e-02\n",
      "   1.57132444e-01   1.45195588e-02   1.30185839e-01   6.78325944e-02\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00  -7.42040506e-02\n",
      "   1.52855330e-01  -7.29695145e-05   3.09203028e-02  -7.23344113e-02\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   8.03789185e-02\n",
      "   3.12332726e-01   1.42979779e-01   1.66312837e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   2.07902310e-01  -1.56464020e-02\n",
      "   1.93784284e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   4.27853552e-01   1.68065373e-01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00  -2.60061853e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "(7227, 105)\n"
     ]
    }
   ],
   "source": [
    "print W2V_words_sim[9]\n",
    "print W2V_words_sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7227, 1)\n",
      "[ 0.68437514]\n"
     ]
    }
   ],
   "source": [
    "print Mean_W2V_feature.shape\n",
    "print Mean_W2V_feature[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7227, 241)\n",
      "[ 0.11588091 -0.56805277  0.40552834 -0.58920974 -0.18696839 -0.88444752\n",
      " -0.76710904  1.06271863  0.13963798 -0.81317729 -0.35954243 -0.3871538\n",
      "  1.58156538  0.05357239 -0.74392366  0.49427027  0.12111622 -0.73700917\n",
      "  0.51665366 -0.33977181  0.15570831  0.00975247  0.60590458  0.12072846\n",
      "  0.65226758 -0.3905386   0.51072025  0.64440018  0.83606666  0.07477744\n",
      " -1.00437212 -0.17038691  0.61684734 -0.06145687  0.34492812 -0.47568724\n",
      " -0.43611073  0.59796011 -0.35167515 -0.60352033  0.35218674  0.55607569\n",
      "  0.05356154 -0.96060181  0.1617336  -0.10417278  0.20310217  0.02744379\n",
      " -0.14758456  0.5230583  -0.27601647  1.61717916  0.61725515  0.02728485\n",
      " -0.88949114 -0.27143988  0.1644586  -0.25284407 -0.03902939 -0.30906001\n",
      "  0.24992552 -0.81360841  0.75376874  1.38346231 -0.92820841 -0.79101408\n",
      " -0.28832433  0.70646673  0.57049316 -0.54769182  0.7079044  -0.94162923\n",
      "  0.05456032  0.00616537  0.2266406  -0.07447936  1.13989067  0.93451488\n",
      "  0.75734794  0.37984902  0.20693979  0.04443424 -0.7964353   1.10616088\n",
      " -1.07811093 -0.00783017  0.10018666  0.02322066 -1.09588432  0.21820635\n",
      "  1.25198388 -0.64916033  0.16747119 -0.14338465 -0.07911533  0.17727548\n",
      "  0.62550795 -1.49636209  0.28828752  0.00271428  0.33539125  0.79308546\n",
      "  0.36779645 -0.176661   -0.20204853  0.28138384  0.83985937 -0.91282773\n",
      "  0.65851706  0.61738366 -0.02251438  0.3854782  -0.07683776  0.41540468\n",
      " -0.04360294  0.0652307   1.6562264   0.65813714 -0.48816925 -0.14197887\n",
      "  0.39833248 -0.87673855  0.20094045 -0.43724227 -0.13724402 -0.86198109\n",
      " -0.95489228  0.69804847 -0.45220655 -1.28885555  0.07895589 -0.08699509\n",
      "  1.70168281  0.34573293 -0.17669983  0.64384228  0.23979382 -0.83227843\n",
      "  0.76751393 -0.10253008 -0.34195918  0.44341022  0.53209651  0.19562925\n",
      "  1.08699393 -0.43407732  0.44462219  0.82771111  0.82330877  0.40328544\n",
      " -1.15431213 -0.26533753  0.64222026 -0.47632903  0.50392956 -0.56285727\n",
      " -0.13228288  0.14900622 -0.03614966 -0.19006583  0.22476134  0.63938743\n",
      " -0.32099438 -0.73954779  0.26902819 -0.18829264  0.10069901 -0.1792773\n",
      "  0.34752038  0.66439945 -0.18348578  1.7310245   0.44209105  0.09611751\n",
      " -0.68190658  0.26918566 -0.06445421  0.1012968   0.00542531  0.1883322\n",
      " -0.02280543 -0.35546502  0.53181779  1.06757319 -0.91705811 -0.62591314\n",
      " -0.55181074  1.05569398  0.15700804 -0.77819169  0.18406418 -1.06739068\n",
      " -0.34057575  0.15805352 -0.02431527 -0.13002901  1.18679261  1.26658499\n",
      "  0.69037944 -0.01474469 -0.68119645 -0.22712263 -0.49754572  0.88864535\n",
      " -0.78455389 -0.2182672   0.0362144  -0.10241015 -1.08643532 -0.21982567\n",
      "  1.29621279 -0.91689223  0.18238685 -0.42811728 -0.01078557  0.17664151\n",
      "  0.20634635 -1.3457495   0.15901703 -0.06963112  0.19947138  1.1830622\n",
      "  0.17040171  0.08909118  0.01728851  0.27033085  0.69785243 -0.66571504\n",
      "  1.1201179   0.67333257 -0.03408045  0.62790346 -0.34946942  0.23876813\n",
      "  0.04347383 -0.13988774  1.92493784  0.56457299 -0.6325897  -0.48657137\n",
      "  0.90325067]\n"
     ]
    }
   ],
   "source": [
    "print Mean_W2V_feature_expand.shape\n",
    "print Mean_W2V_feature_expand[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String+IR+cosine_MeanW2V features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = np.asarray(Paraphrases[['class']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7227, 19)\n"
     ]
    }
   ],
   "source": [
    "all_features = np.concatenate((string_features, IR_features, Mean_W2V_feature), axis = 1)\n",
    "print all_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7227, 15)\n",
      "(7227, 3)\n"
     ]
    }
   ],
   "source": [
    "print np.asarray(string_features).shape\n",
    "print np.asarray(IR_features).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Str_IR_W2V_model = GradientBoostingClassifier(loss = 'deviance')\n",
    "parametrs_grid = {'learning_rate':[0.005, 0.01, 0.05], 'n_estimators': [100, 300, 500, 700, 1000, 1300],\\\n",
    "                 'subsample':[0.6, 0.7, 0.8, 1.], 'max_depth':[2, 3, 4, 5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 288 candidates, totalling 1728 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:   32.4s\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed: 29.5min\n",
      "[Parallel(n_jobs=5)]: Done 790 tasks      | elapsed: 56.1min\n",
      "[Parallel(n_jobs=5)]: Done 1240 tasks      | elapsed: 96.7min\n",
      "[Parallel(n_jobs=5)]: Done 1728 out of 1728 | elapsed: 140.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=6, error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=100, presort='auto', random_state=None,\n",
       "              subsample=1.0, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=5,\n",
       "       param_grid={'n_estimators': [100, 300, 500, 700, 1000, 1300], 'subsample': [0.6, 0.7, 0.8, 1.0], 'learning_rate': [0.005, 0.01, 0.05], 'max_depth': [2, 3, 4, 5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=make_scorer(f1_score), verbose=1)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_metric = make_scorer(f1_score)\n",
    "grid_model = GridSearchCV(Str_IR_W2V_model, parametrs_grid, cv = 6, verbose = 1, n_jobs=5, scoring=f1_metric)\n",
    "grid_model.fit(all_features, target.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 300, 'subsample': 0.8}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST Gradient Boosting Result on String+IR Features+cosine_MeanW2V features\n",
      "\n",
      "F1-score 0.796789492886\n",
      "Accuracy: 0.710498960499\n"
     ]
    }
   ],
   "source": [
    "best_GB_IR_Str_W2V = GradientBoostingClassifier(loss = 'deviance', learning_rate=0.01, \\\n",
    "                                     max_depth=5, n_estimators=700, subsample=0.8)\n",
    "best_GB_IR_Str_W2V.fit(all_features, target.ravel())\n",
    "\n",
    "test_predict = best_GB_IR_Str_W2V.predict(test_all_features)\n",
    "print \"BEST Gradient Boosting Result on String+IR Features+cosine_MeanW2V features\"\n",
    "print \n",
    "print \"F1-score\", f1_score(np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict)\n",
    "print \"Accuracy:\", accuracy_score(np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03389919  0.01570628  0.01230472  0.02391624  0.01903385  0.01221004\n",
      "  0.0268069   0.01703021  0.01684482  0.07053947  0.11633569  0.0450963\n",
      "  0.04969544  0.06067572  0.05067313  0.22149615  0.02596625  0.08398942\n",
      "  0.09778017]\n"
     ]
    }
   ],
   "source": [
    "print best_GB_IR_Str_W2V.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String+IR+cosine_MeanW2V features+MeanEmbeddingOfSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7227, 259)\n"
     ]
    }
   ],
   "source": [
    "all_features = np.concatenate((string_features, IR_features, Mean_W2V_feature_expand), axis = 1)\n",
    "print all_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Str_IR_W2V_expand_model = GradientBoostingClassifier(loss = 'deviance')\n",
    "parametrs_grid = {'learning_rate':[0.01, 0.05], 'n_estimators': [100, 300, 500, 700],\\\n",
    "                 'subsample':[0.6, 0.7, 0.8, 1.], 'max_depth':[2, 3, 4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 96 candidates, totalling 576 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed: 111.6min\n"
     ]
    }
   ],
   "source": [
    "f1_metric = make_scorer(f1_score)\n",
    "grid_model = GridSearchCV(Str_IR_W2V_expand_model, parametrs_grid, cv = 6, verbose = 1, n_jobs=5, scoring=f1_metric)\n",
    "grid_model.fit(all_features, target.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 300, 'subsample': 0.7}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST Gradient Boosting Result on String+IR Features+cosine_MeanW2V features\n",
      "\n",
      "F1-score 0.797238372093\n",
      "Accuracy: 0.709979209979\n"
     ]
    }
   ],
   "source": [
    "best_GB_IR_Str_W2V = GradientBoostingClassifier(loss = 'deviance', learning_rate=0.01, \\\n",
    "                                     max_depth=2, n_estimators=300, subsample=0.7)\n",
    "best_GB_IR_Str_W2V.fit(all_features, target.ravel())\n",
    "\n",
    "test_predict = best_GB_IR_Str_W2V.predict(test_all_features_expand)\n",
    "print \"BEST Gradient Boosting Result on String+IR Features+cosine_MeanW2V features\"\n",
    "print \n",
    "print \"F1-score\", f1_score(np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict)\n",
    "print \"Accuracy:\", accuracy_score(np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  8.60579913e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   3.79584441e-04   1.50269261e-03   0.00000000e+00   7.91349489e-05\n",
      "   2.16669888e-04   9.64157408e-02   1.63191722e-01   1.27031353e-02\n",
      "   8.50628635e-03   5.12466397e-02   3.34716728e-02   3.06962102e-01\n",
      "   0.00000000e+00   4.70760470e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   6.47674241e-04   0.00000000e+00   0.00000000e+00   3.34610795e-03\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   6.04896212e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.24873243e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   3.40625049e-04   0.00000000e+00   4.20146167e-03   0.00000000e+00\n",
      "   0.00000000e+00   7.01181852e-04   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   2.56025930e-04   0.00000000e+00   0.00000000e+00\n",
      "   4.11787284e-04   0.00000000e+00   1.10962014e-03   0.00000000e+00\n",
      "   2.32470517e-04   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   3.38813550e-04\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   3.30874489e-04   5.88990185e-03   1.05265297e-03\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   3.91444485e-04   1.16919705e-03\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   4.79225015e-04\n",
      "   0.00000000e+00   2.93709605e-03   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.55265406e-02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   9.88231887e-04   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.96723403e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   7.18354729e-04   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   2.76492983e-04   0.00000000e+00\n",
      "   0.00000000e+00   6.54377932e-03   0.00000000e+00   1.42132434e-03\n",
      "   2.24300634e-03   0.00000000e+00   4.80164000e-04   1.38995825e-02\n",
      "   0.00000000e+00   0.00000000e+00   1.39024366e-03   0.00000000e+00\n",
      "   1.12199781e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   8.26284171e-04   0.00000000e+00   6.90630257e-04   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   4.99917967e-04\n",
      "   0.00000000e+00   9.25464191e-04   2.08327542e-03   3.44391724e-04\n",
      "   3.62234024e-04   0.00000000e+00   0.00000000e+00   3.73739479e-04\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   5.62799663e-04   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.11059226e-03   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   6.55755882e-04   2.25485935e-03\n",
      "   0.00000000e+00   0.00000000e+00   3.73322999e-03   4.69055303e-04\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   3.87640961e-04   0.00000000e+00   7.10369000e-04\n",
      "   0.00000000e+00   6.24672606e-04   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.98434985e-03   1.06297378e-02   4.04510527e-02\n",
      "   9.97884332e-04   4.70679451e-03   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   3.06556566e-03   0.00000000e+00   0.00000000e+00   1.14052174e-03\n",
      "   0.00000000e+00   9.26179934e-03   0.00000000e+00   0.00000000e+00\n",
      "   7.07486667e-04   5.12870711e-03   0.00000000e+00   0.00000000e+00\n",
      "   4.38743741e-04   0.00000000e+00   1.47015213e-03   2.06497465e-03\n",
      "   5.21357584e-04   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   2.01291734e-03   0.00000000e+00   0.00000000e+00   3.59835026e-03\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   3.41506750e-04   8.67951518e-02]\n"
     ]
    }
   ],
   "source": [
    "print best_GB_IR_Str_W2V.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String + IR + W2V_max_similatity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7227, 20)\n",
      "(1924, 20)\n"
     ]
    }
   ],
   "source": [
    "all_features = np.concatenate((string_features, IR_features, W2V_max_sim_feature), axis = 1)\n",
    "print all_features.shape\n",
    "\n",
    "test_all_features = np.concatenate((test_string_features, test_IR_features,\\\n",
    "                                    test_max_sim_W2V), axis = 1)\n",
    "print test_all_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Str_IR_W2V_sim_model = GradientBoostingClassifier()\n",
    "parametrs_grid = {'learning_rate':[0.01, 0.05], 'n_estimators': [100, 300, 500, 700, 1000, 1200],\\\n",
    "                 'subsample':[0.6, 0.7, 0.8, 1.], 'max_depth':[2, 3, 4, 5, 6]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 240 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:   33.7s\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed: 29.1min\n",
      "[Parallel(n_jobs=5)]: Done 790 tasks      | elapsed: 73.4min\n",
      "[Parallel(n_jobs=5)]: Done 1240 tasks      | elapsed: 107.3min\n",
      "[Parallel(n_jobs=5)]: Done 1440 out of 1440 | elapsed: 143.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=6, error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=100, presort='auto', random_state=None,\n",
       "              subsample=1.0, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=5,\n",
       "       param_grid={'n_estimators': [100, 300, 500, 700, 1000, 1200], 'subsample': [0.6, 0.7, 0.8, 1.0], 'learning_rate': [0.01, 0.05], 'max_depth': [2, 3, 4, 5, 6]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=make_scorer(f1_score), verbose=1)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_metric = make_scorer(f1_score)\n",
    "grid_model = GridSearchCV(Str_IR_W2V_sim_model, parametrs_grid, cv = 6, verbose = 1, n_jobs=5, scoring=f1_metric)\n",
    "grid_model.fit(all_features, target.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 100, 'subsample': 0.7}"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST Gradient Boosting Result on String+IR Features+cosine_MeanW2V features\n",
      "\n",
      "F1-score 0.804724990772\n",
      "Accuracy: 0.725051975052\n"
     ]
    }
   ],
   "source": [
    "best_GB_IR_Str_W2V = GradientBoostingClassifier(loss = 'deviance', learning_rate=0.01, \\\n",
    "                                     max_depth=3, n_estimators=900, subsample=0.7)\n",
    "best_GB_IR_Str_W2V.fit(all_features, target.ravel())\n",
    "\n",
    "test_predict = best_GB_IR_Str_W2V.predict(test_all_features)\n",
    "print \"BEST Gradient Boosting Result on String+IR Features+cosine_MeanW2V features\"\n",
    "print \n",
    "print \"F1-score\", f1_score(np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict)\n",
    "print \"Accuracy:\", accuracy_score(np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03968404  0.01541248  0.01104407  0.02452416  0.0200672   0.01319009\n",
      "  0.02631749  0.01592034  0.01681601  0.07457444  0.1003791   0.04218479\n",
      "  0.05145018  0.05877924  0.05359984  0.21048694  0.02186215  0.09011878\n",
      "  0.00027522  0.11331345]\n"
     ]
    }
   ],
   "source": [
    "print best_GB_IR_Str_W2V.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST Gradient Boosting Result on String+IR Features+cosine_MeanW2V features\n",
      "\n",
      "F1-score 0.803571428571\n",
      "Accuracy: 0.725571725572\n"
     ]
    }
   ],
   "source": [
    "best_GB_IR_Str_W2V = RandomForestClassifier(n_estimators=2000)\n",
    "best_GB_IR_Str_W2V.fit(all_features, target.ravel())\n",
    "\n",
    "test_predict = best_GB_IR_Str_W2V.predict(test_all_features)\n",
    "print \"BEST Gradient Boosting Result on String+IR Features+cosine_MeanW2V features\"\n",
    "print \n",
    "print \"F1-score\", f1_score(np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict)\n",
    "print \"Accuracy:\", accuracy_score(np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST Gradient Boosting Result on String+IR Features+cosine_MeanW2V features\n",
      "\n",
      "F1-score 0.77401894452\n",
      "Accuracy: 0.652806652807\n"
     ]
    }
   ],
   "source": [
    "best_GB_IR_Str_W2V = svm.LinearSVC()\n",
    "best_GB_IR_Str_W2V.fit(all_features, target.ravel())\n",
    "\n",
    "test_predict = best_GB_IR_Str_W2V.predict(test_all_features)\n",
    "print \"BEST Gradient Boosting Result on String+IR Features+cosine_MeanW2V features\"\n",
    "print \n",
    "print \"F1-score\", f1_score(np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict)\n",
    "print \"Accuracy:\", accuracy_score(np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.05158023  0.0236062   0.01348949  0.03343941  0.02100725  0.01227461\n",
      "  0.04358154  0.02176164  0.01202767  0.09862912  0.11057972  0.05374822\n",
      "  0.07414088  0.06618     0.08941057  0.13680704  0.02455947  0.05430478\n",
      "  0.0007671   0.05810507]\n"
     ]
    }
   ],
   "source": [
    "print best_GB_IR_Str_W2V.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String + IR + W2V_max_similatity + WordsSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7227, 125)\n",
      "(1924, 125)\n"
     ]
    }
   ],
   "source": [
    "all_features = np.concatenate((string_features, IR_features, W2V_max_sim_feature, W2V_words_sim), axis = 1)\n",
    "print all_features.shape\n",
    "\n",
    "test_all_features = np.concatenate((test_string_features, test_IR_features,\\\n",
    "                                    test_max_sim_W2V, test_W2V_words_sim), axis = 1)\n",
    "print test_all_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sf 9 34\n",
      "sf 9 35\n",
      "sf 9 36\n",
      "sf 9 37\n",
      "sf 9 38\n",
      "sf 9 39\n",
      "sf 26 21\n",
      "sf 26 34\n",
      "sf 30 24\n",
      "sf 30 37\n",
      "sf 30 49\n",
      "sf 30 60\n",
      "sf 30 70\n",
      "sf 31 24\n",
      "sf 31 37\n",
      "sf 31 49\n",
      "sf 31 60\n",
      "sf 31 70\n",
      "sf 38 70\n",
      "sf 38 71\n",
      "sf 38 72\n",
      "sf 38 73\n",
      "sf 38 74\n",
      "sf 43 97\n",
      "sf 43 98\n",
      "sf 47 89\n",
      "sf 47 90\n",
      "sf 75 34\n",
      "sf 75 35\n",
      "sf 75 36\n",
      "sf 75 37\n",
      "sf 75 38\n",
      "sf 76 70\n",
      "sf 76 71\n",
      "sf 86 89\n",
      "sf 87 20\n",
      "sf 87 21\n",
      "sf 87 22\n",
      "sf 87 23\n",
      "sf 87 24\n",
      "sf 88 89\n",
      "sf 88 90\n",
      "sf 88 91\n",
      "sf 98 80\n",
      "sf 98 81\n",
      "sf 100 70\n",
      "sf 100 71\n",
      "sf 100 72\n",
      "sf 100 73\n",
      "sf 103 70\n",
      "sf 113 22\n",
      "sf 113 35\n",
      "sf 113 47\n",
      "sf 113 59\n",
      "sf 113 60\n",
      "sf 113 61\n",
      "sf 113 62\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-287-e1a4aed27acc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m\"sf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(np.isnan(all_features).shape[0]):\n",
    "    for j in range(np.isnan(all_features).shape[1]):\n",
    "        if np.isnan(all_features)[i, j] == True:\n",
    "            print \"sf\", i, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST Gradient Boosting Result on String+IR Features+cosine_MeanW2V features\n",
      "\n",
      "F1-score 0.807692307692\n",
      "Accuracy: 0.72972972973\n"
     ]
    }
   ],
   "source": [
    "best_GB_IR_Str_W2V = GradientBoostingClassifier(loss = 'deviance', learning_rate=0.01, \\\n",
    "                                     max_depth=4, n_estimators=800, subsample=0.7)\n",
    "best_GB_IR_Str_W2V.fit(all_features, target.ravel())\n",
    "\n",
    "test_predict = best_GB_IR_Str_W2V.predict(test_all_features)\n",
    "print \"BEST Gradient Boosting Result on String+IR Features+cosine_MeanW2V features\"\n",
    "print \n",
    "print \"F1-score\", f1_score(np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict)\n",
    "print \"Accuracy:\", accuracy_score(np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.85828185e-02   6.42665313e-03   5.52733390e-03   1.31589323e-02\n",
      "   9.07820943e-03   5.97622261e-03   8.58968710e-03   9.70602178e-03\n",
      "   1.04071710e-02   4.30864905e-02   6.46184755e-02   2.38911618e-02\n",
      "   1.99846501e-02   2.81375195e-02   1.75330051e-02   1.49365349e-01\n",
      "   5.71618389e-03   3.01676291e-02   7.11480013e-10   5.59637597e-02\n",
      "   1.39559767e-02   1.25586951e-02   1.42780264e-02   2.16139876e-02\n",
      "   1.56586039e-02   1.89308134e-02   8.96589859e-03   8.49880295e-03\n",
      "   3.95705598e-03   6.55642318e-03   4.78722360e-04   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   9.77746000e-03   1.47752880e-02\n",
      "   1.55572489e-02   1.65616433e-02   2.00659732e-02   1.08428855e-02\n",
      "   5.40351291e-03   1.86331245e-03   6.54860897e-04   2.69493884e-04\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.22381241e-02\n",
      "   1.15436043e-02   1.65891541e-02   1.00580569e-02   1.02203539e-02\n",
      "   7.30045500e-03   3.78007762e-03   3.37342128e-03   2.87263535e-04\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.04265631e-02\n",
      "   1.24487357e-02   1.03034900e-02   8.86736434e-03   8.09926462e-03\n",
      "   2.22684111e-03   1.08293839e-03   1.15118696e-03   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   8.48174481e-03   1.43931791e-02\n",
      "   8.21404915e-03   1.04881178e-02   7.77190823e-03   5.61668868e-03\n",
      "   1.07000512e-04   0.00000000e+00   0.00000000e+00   1.58214892e-10\n",
      "   7.90894759e-03   4.85373553e-03   7.31842551e-03   4.79183143e-03\n",
      "   2.52901085e-03   4.86154673e-04   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.42913741e-02   5.13337587e-03   2.60847115e-03\n",
      "   3.18308209e-03   3.83100042e-04   9.29768908e-06   0.00000000e+00\n",
      "   0.00000000e+00   4.63547029e-03   2.68817796e-03   6.60008640e-04\n",
      "   2.51499068e-04   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   2.01990668e-03   2.46862827e-03   8.96125052e-04   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   2.13601753e-04   3.42708568e-05\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   4.23961112e-04\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print best_GB_IR_Str_W2V.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST Gradient Boosting Result on String+IR Features+cosine_MeanW2V features\n",
      "\n",
      "F1-score 0.805880135695\n",
      "Accuracy: 0.732328482328\n"
     ]
    }
   ],
   "source": [
    "best_GB_IR_Str_W2V = RandomForestClassifier(n_estimators=2000)\n",
    "best_GB_IR_Str_W2V.fit(all_features, target.ravel())\n",
    "\n",
    "test_predict = best_GB_IR_Str_W2V.predict(test_all_features)\n",
    "print \"BEST Gradient Boosting Result on String+IR Features+cosine_MeanW2V features\"\n",
    "print \n",
    "print \"F1-score\", f1_score(np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict)\n",
    "print \"Accuracy:\", accuracy_score(np.asarray(np_test_data[:, -1], dtype= np.int64), test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "поздравить\n",
      "год\n",
      "с\n",
      "год\n",
      "за\n",
      "19\n",
      "новый\n",
      "гражданин\n",
      "северокорейский\n",
      "лидер\n",
      "впервые\n",
      "\n",
      "лидер\n",
      "кндр\n",
      "впервые\n",
      "за\n",
      "19\n",
      "год\n",
      "поздравить\n",
      "согражданин\n",
      "с\n",
      "новый\n",
      "год\n"
     ]
    }
   ],
   "source": [
    "sent1 = forward_index[np_paraphrases[13][1]]\n",
    "sent2 = forward_index[np_paraphrases[13][2]]\n",
    "bprint(sent1)\n",
    "# bprint(r.shuffle(sent1))\n",
    "print\n",
    "bprint(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "кндр\n",
      "лидер\n",
      "за\n",
      "впервые\n",
      "год\n",
      "19\n",
      "согражданин\n",
      "поздравить\n",
      "новый\n",
      "с\n",
      "год\n"
     ]
    }
   ],
   "source": [
    "t = np.asarray(sent2)\n",
    "for i in range(0, len(t) - 1, 2):\n",
    "    k = t[i]\n",
    "    t[i] = t[i+1]\n",
    "    t[i+1] = k\n",
    "    \n",
    "bprint(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np_paraphrases_advanced = np.concatenate((np_paraphrases, np_paraphrases * -1), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14454, 3)\n",
      "12062\n"
     ]
    }
   ],
   "source": [
    "print np_paraphrases_advanced.shape\n",
    "print len(forward_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forward_index_advanced = forward_index.copy()\n",
    "for sent_id1, sent_id2 in np_paraphrases_advanced[len(np_paraphrases):, 1:]:\n",
    "    if sent_id1 < 0:\n",
    "        sent = list(forward_index[sent_id1 * -1])\n",
    "#         bprint(sent)\n",
    "        for i in range(0, len(sent) - 1, 2):\n",
    "            k = sent[i]\n",
    "            sent[i] = sent[i+1]\n",
    "            sent[i+1] = k\n",
    "#         bprint(sent)\n",
    "        forward_index_advanced[sent_id1] = sent\n",
    "        \n",
    "    if sent_id2 < 0:\n",
    "        sent = forward_index[sent_id2 * -1]\n",
    "        for i in range(0, len(sent) - 1, 2):\n",
    "            k = sent[i]\n",
    "            sent[i] = sent[i+1]\n",
    "            sent[i+1] = k\n",
    "        forward_index_advanced[sent_id2] = sent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7227\n"
     ]
    }
   ],
   "source": [
    "print len(np_paraphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1  201 8159]\n"
     ]
    }
   ],
   "source": [
    "print np_paraphrases_advanced[0]\n",
    "print np_paraphrases_advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24124\n",
      "разрешить\n",
      "полицейский\n",
      "на\n",
      "стрелять\n",
      "по\n",
      "поражение\n",
      "с\n",
      "гражданин\n",
      "травматика\n",
      "\n",
      "полицейский\n",
      "разрешить\n",
      "стрелять\n",
      "на\n",
      "поражение\n",
      "по\n",
      "гражданин\n",
      "с\n",
      "травматика\n"
     ]
    }
   ],
   "source": [
    "print len(forward_index_advanced.keys())\n",
    "bprint(forward_index_advanced[201])\n",
    "print \n",
    "bprint(forward_index_advanced[-201])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
